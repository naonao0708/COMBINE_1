{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.metrics import SensitivityAtSpecificity\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, matthews_corrcoef\n",
    "from sklearn.preprocessing import Binarizer\n",
    "from imblearn.over_sampling import ADASYN, BorderlineSMOTE\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    roc_auc_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    matthews_corrcoef,\n",
    "    confusion_matrix,\n",
    "    roc_curve\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "def calculate_evaluation_metrics(y_true, y_pred_probs, target_specificity):\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_pred_probs)\n",
    "    specificity = 1 - fpr\n",
    "    # 找到最接近目标特异性的阈值索引\n",
    "    closest_index = np.argmin(np.abs(specificity - target_specificity))\n",
    "    best_threshold = thresholds[closest_index]\n",
    "    y_pred = y_pred_probs >= best_threshold\n",
    "    \n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    auc = roc_auc_score(y_true, y_pred_probs)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    sn = recall_score(y_true, y_pred)\n",
    "    mcc = matthews_corrcoef(y_true, y_pred)\n",
    "    \n",
    "    return acc, auc, precision, sn, mcc, best_threshold\n",
    "\n",
    "def calculate(y_true, y_pred_probs):\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred_probs = np.array(y_pred_probs)\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_pred_probs)\n",
    "    specificity = 1 - fpr\n",
    "\n",
    "\n",
    "    best_threshold = 0.5\n",
    "    y_pred = y_pred_probs >= best_threshold\n",
    "    \n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    auc = roc_auc_score(y_true, y_pred_probs)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    sn = recall_score(y_true, y_pred)\n",
    "    mcc = matthews_corrcoef(y_true, y_pred)\n",
    "    \n",
    "    return acc, auc, precision, sn, mcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RACE_Samples = pd.read_csv(\"TCGA_RACE.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "WHITE = RACE_Samples[RACE_Samples[\"RACE\"] == \"WHITE\"][\"A0_Samples\"].values\n",
    "BLACK = RACE_Samples[RACE_Samples[\"RACE\"] == \"BLACK\"][\"A0_Samples\"].values\n",
    "all_sim = set(pd.read_csv(r\"最终数据/tcga_clinical.csv\", index_col=0).T.columns)\n",
    "WHITE = list(all_sim & set(WHITE))\n",
    "BLACK = list(all_sim & set(BLACK))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicate_columns(df):\n",
    "    \"\"\"\n",
    "    移除DataFrame中的重复列名，只保留第一次出现的列。\n",
    "\n",
    "    参数:\n",
    "    df: pd.DataFrame, 原始DataFrame。\n",
    "\n",
    "    返回:\n",
    "    df_unique_columns: pd.DataFrame, 移除重复列名后的DataFrame。\n",
    "    \"\"\"\n",
    "    # 查找重复的列名\n",
    "    duplicated_columns = df.columns.duplicated()\n",
    "\n",
    "    # 选择不重复的列\n",
    "    df_unique_columns = df.loc[:, ~duplicated_columns]\n",
    "    \n",
    "    return df_unique_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "TCGA_cna = pd.read_csv(\"gene.csv\",index_col=0)\n",
    "TCGA_gene = pd.read_csv(\"cna.csv\",index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "TCGA_cna = remove_duplicate_columns(TCGA_cna.T).T\n",
    "TCGA_gene = remove_duplicate_columns(TCGA_gene.T).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "TCGA_gene_WHITE = TCGA_gene.T[WHITE].T\n",
    "TCGA_gene_BLACK = TCGA_gene.T[BLACK].T\n",
    "\n",
    "TCGA_cna_WHITE = TCGA_cna.T[WHITE].T\n",
    "TCGA_cna_BLACK = TCGA_cna.T[BLACK].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.svm import LinearSVC\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def feature_selection_linear_svc(X, y ,fea_num=100):\n",
    "    # Initialize LinearSVC with L1 penalty, which is suitable for feature selection\n",
    "    lsvc = LinearSVC(max_iter=1000000, penalty=\"l1\", dual=False)\n",
    "\n",
    "    # Initialize SelectFromModel using LinearSVC\n",
    "    selector = SelectFromModel(lsvc, max_features=fea_num)\n",
    "    selector.fit(X, y)\n",
    "    \n",
    "    # Get the mask of the selected features\n",
    "    support_mask = selector.get_support()\n",
    "    \n",
    "    # Return the DataFrame with the selected features and the mask\n",
    "    return X.loc[:, support_mask]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "TCGA_label_BLACK = TCGA_label_BLACK.astype(int)\n",
    "TCGA_gene_BLACK_65 = feature_selection_linear_svc(TCGA_gene_BLACK, TCGA_label_BLACK, fea_num=65)\n",
    "\n",
    "TCGA_cna_BLACK_100 = feature_selection_linear_svc(TCGA_cna_BLACK, TCGA_label_BLACK, fea_num=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "TCGA_label_WHITE  = TCGA_label_WHITE.astype(int)\n",
    "TCGA_gene_WHITE_280 = feature_selection_linear_svc(TCGA_gene_WHITE, TCGA_label_WHITE, fea_num=280)\n",
    "\n",
    "TCGA_cna_WHITE_280 = feature_selection_linear_svc(TCGA_cna_WHITE, TCGA_label_WHITE, fea_num=280)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "TCGA_clinical_WHITE = pd.read_csv(r\"最终数据/tcga_clinical.csv\", index_col=0).T[WHITE]\n",
    "TCGA_clinical_BLACK = pd.read_csv(r\"最终数据/tcga_clinical.csv\", index_col=0).T[BLACK]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "TCGA_gene_BLACK_65.to_csv(\"TCGA_gene_BLACK_65.csv\")\n",
    "TCGA_cna_BLACK_100.to_csv(\"TCGA_cna_BLACK_100.csv\")\n",
    "\n",
    "TCGA_gene_WHITE_280.to_csv(\"TCGA_gene_WHITE_280.csv\")\n",
    "TCGA_cna_WHITE_280.to_csv(\"TCGA_cna_WHITE_280.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "skf = StratifiedKFold(n_splits=5,shuffle=True, random_state=42)\n",
    "\n",
    "probabilities_gene = []\n",
    "probabilities_cna = []\n",
    "probabilities_clin = []\n",
    "true_labels = []\n",
    "\n",
    "for train_index, test_index in skf.split(TCGA_gene_WHITE_280.values, TCGA_label_WHITE):\n",
    "    \n",
    "    X_train, X_test = TCGA_gene_WHITE_280.values[train_index], TCGA_gene_WHITE_280.values[test_index]\n",
    "    y_train, y_test = TCGA_label_WHITE[train_index], TCGA_label_WHITE[test_index]\n",
    "    lsvc = LogisticRegression(max_iter=1000000)\n",
    "    lsvc.fit(X_train, y_train)\n",
    "    proba = lsvc.predict_proba(X_test)[:, 1]  # 获取正类的概率\n",
    "    probabilities_gene.extend(proba)\n",
    "    \n",
    "    # 保存真实标签\n",
    "    \n",
    "    X_train, X_test = TCGA_cna_WHITE_280.values[train_index], TCGA_cna_WHITE_280.values[test_index]\n",
    "    lsvc = LogisticRegression(max_iter=1000000)\n",
    "    lsvc.fit(X_train, y_train)\n",
    "    proba = lsvc.predict_proba(X_test)[:, 1]  # 获取正类的概率\n",
    "    probabilities_cna.extend(proba)\n",
    "\n",
    "\n",
    "    \n",
    "    X_train, X_test = TCGA_clinical_WHITE.T.values[train_index], TCGA_clinical_WHITE.T.values[test_index]\n",
    "    lsvc = LogisticRegression(max_iter=1000000)\n",
    "    lsvc.fit(X_train, y_train)\n",
    "    proba = lsvc.predict_proba(X_test)[:, 1]  # 获取正类的概率\n",
    "    probabilities_clin.extend(proba)\n",
    "    \n",
    "    true_labels.extend(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([probabilities_clin,probabilities_cna, probabilities_gene,true_labels],index=[\"clincal\",\"cna\",\"gene\",\"trues\"]).T.to_csv(\"白人结果.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "data = pd.read_csv(\"黑人结果.csv\", index_col=0)\n",
    "X = data[[\"clincal\", \"cna\", \"gene\"]].values\n",
    "y = data[\"trues\"].values.astype(int)\n",
    "\n",
    "rf_preds = []\n",
    "\n",
    "true_labels = []\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=False)\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    \n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Random Forest\n",
    "    rf_model = RandomForestClassifier(criterion='entropy', max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=100)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    rf_preds.extend(list(rf_model.predict_proba(X_test)[:, 1]))\n",
    "    true_labels.extend(list(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8674033149171271, 0.8103802672147995, 0.8214285714285714, 0.5476190476190477, 0.5972806513587573)\n",
      "(0.9171270718232044, 0.9837273038711888, 0.9090909090909091, 0.7142857142857143, 0.7573406602864717)\n",
      "(0.994475138121547, 0.999828708461802, 0.9767441860465116, 1.0, 0.9847422248772766)\n",
      "(0.994475138121547, 0.9998287084618019, 1.0, 0.9761904761904762, 0.9844885408550117)\n"
     ]
    }
   ],
   "source": [
    "data[[\"clincal\", \"cna\", \"gene\"]].values\n",
    "print(calculate(y, data[\"clincal\"].values))\n",
    "print(calculate(y, data[\"cna\"].values))\n",
    "print(calculate(y, data[\"gene\"].values))\n",
    "\n",
    "print(calculate(true_labels, rf_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "skf = StratifiedKFold(n_splits=5,shuffle=True, random_state=42)\n",
    "\n",
    "probabilities_gene = []\n",
    "probabilities_cna = []\n",
    "probabilities_clin = []\n",
    "true_labels = []\n",
    "\n",
    "for train_index, test_index in skf.split(TCGA_gene_BLACK_280.values, TCGA_label_BLACK):\n",
    "    \n",
    "    X_train, X_test = TCGA_gene_BLACK_65.values[train_index], TCGA_gene_BLACK_65.values[test_index]\n",
    "    y_train, y_test = TCGA_label_BLACK[train_index], TCGA_label_BLACK[test_index]\n",
    "    lsvc = LogisticRegression(max_iter=1000000)\n",
    "    lsvc.fit(X_train, y_train)\n",
    "    proba = lsvc.predict_proba(X_test)[:, 1]  # 获取正类的概率\n",
    "    probabilities_gene.extend(proba)\n",
    "    \n",
    "    # 保存真实标签\n",
    "    \n",
    "    X_train, X_test = TCGA_cna_BLACK_100.values[train_index], TCGA_cna_BLACK_100.values[test_index]\n",
    "    lsvc = LogisticRegression(max_iter=1000000)\n",
    "    lsvc.fit(X_train, y_train)\n",
    "    proba = lsvc.predict_proba(X_test)[:, 1]  # 获取正类的概率\n",
    "    probabilities_cna.extend(proba)\n",
    "\n",
    "\n",
    "    \n",
    "    X_train, X_test = TCGA_clinical_BLACK.T.values[train_index], TCGA_clinical_BLACK.T.values[test_index]\n",
    "    lsvc = LogisticRegression(max_iter=1000000)\n",
    "    lsvc.fit(X_train, y_train)\n",
    "    proba = lsvc.predict_proba(X_test)[:, 1]  # 获取正类的概率\n",
    "    probabilities_clin.extend(proba)\n",
    "    \n",
    "    true_labels.extend(y_test)\n",
    "|728788"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "del TCGA_gene[\"?|728788\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x684 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 导入所需的库\n",
    "import shap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# 假设你已经有了数据集 X 和目标变量 y\n",
    "X = TCGA_gene  # 特征矩阵\n",
    "y = TCGA_label # 目标变量\n",
    "\n",
    "# 分割数据集以进行训练和测试\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 创建并训练逻辑回归模型\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 初始化SHAP解释器\n",
    "explainer = shap.Explainer(model, X_train)\n",
    "\n",
    "# 计算SHAP值\n",
    "shap_values = explainer(X_test)\n",
    "\n",
    "# 生成可视化图表\n",
    "\n",
    "## 水流图（Waterfall plot）- 展示单个预测的贡献\n",
    "shap.plots.waterfall(shap_values[0], show=False)\n",
    "plt.savefig('ALL_waterfall_plot.png', dpi=300, format='png')\n",
    "plt.clf()  # 清除当前图像\n",
    "\n",
    "## 摘要图（Summary plot）- 展示所有特征对模型的整体贡献\n",
    "shap.summary_plot(shap_values, X_test, show=False)\n",
    "plt.savefig('ALL_Summary_plot.png', dpi=300, format='png')\n",
    "# ## 依赖关系图（Dependence plot）- 展示特征与模型输出之间的关系\n",
    "# for feature_name in X.columns:\n",
    "#     shap.dependence_plot(feature_name, shap_values.values, X_test)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([probabilities_clin,probabilities_cna, probabilities_gene,true_labels],index=[\"clincal\",\"cna\",\"gene\",\"trues\"]).T.to_csv(\"黑人结果.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8674033149171271, 0.8103802672147995, 0.8214285714285714, 0.5476190476190477, 0.5972806513587573)\n",
      "(0.9171270718232044, 0.9837273038711888, 0.9090909090909091, 0.7142857142857143, 0.7573406602864717)\n",
      "(0.994475138121547, 0.999828708461802, 0.9767441860465116, 1.0, 0.9847422248772766)\n"
     ]
    }
   ],
   "source": [
    "print(calculate(true_labels, probabilities_clin))\n",
    "print(calculate(true_labels, probabilities_cna))\n",
    "print(calculate(true_labels, probabilities_gene))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for each fold:  [0.99328859 0.99328859 0.97297297 0.98648649 0.97972973]\n",
      "AUC for each fold:  [1.         0.99977064 1.         0.99976476 0.99929428]\n",
      "Precision for each fold:  [1.         1.         1.         1.         0.97368421]\n",
      "Recall for each fold:  [0.97435897 0.975      0.8974359  0.94871795 0.94871795]\n",
      "MCC for each fold:  [0.98263979 0.98292236 0.93041295 0.96520668 0.94748942]\n",
      "\n",
      "Average accuracy:  0.9851532740794486 Standard Deviation:  0.007898338731533481\n",
      "Average AUC:  0.9997659374264878 Standard Deviation:  0.00025770189546126486\n",
      "Average precision:  0.9947368421052631 Standard Deviation:  0.010526315789473672\n",
      "Average recall:  0.9488461538461539 Standard Deviation:  0.02820629368221408\n",
      "Average MCC:  0.9617342391563053 Standard Deviation:  0.02040575766021115\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, matthews_corrcoef, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X = TCGA_gene_WHITE_280.values\n",
    "y = TCGA_label_WHITE\n",
    "\n",
    "# 初始化 LinearSVC 模型\n",
    "lsvc = LogisticRegression(max_iter=1000000)\n",
    "\n",
    "# 使用 StratifiedKFold 进行分层抽样\n",
    "cv = StratifiedKFold(n_splits=5)\n",
    "\n",
    "# 定义评分指标\n",
    "scoring = {\n",
    "    'acc': 'accuracy',\n",
    "    'auc': make_scorer(roc_auc_score, needs_threshold=True),\n",
    "    'pre': 'precision',\n",
    "    'rec': 'recall',\n",
    "    'mcc': make_scorer(matthews_corrcoef)\n",
    "}\n",
    "\n",
    "# 进行交叉验证\n",
    "scores = cross_validate(lsvc, X, y, cv=cv, scoring=scoring, return_train_score=False, return_estimator=True)\n",
    "\n",
    "# 打印每一折的评分指标\n",
    "print(\"Accuracy for each fold: \", scores['test_acc'])\n",
    "print(\"AUC for each fold: \", scores['test_auc'])\n",
    "print(\"Precision for each fold: \", scores['test_pre'])\n",
    "print(\"Recall for each fold: \", scores['test_rec'])\n",
    "print(\"MCC for each fold: \", scores['test_mcc'])\n",
    "\n",
    "# 打印每个评分指标的平均值和标准偏差\n",
    "print(\"\\nAverage accuracy: \", scores['test_acc'].mean(), \"Standard Deviation: \", scores['test_acc'].std())\n",
    "print(\"Average AUC: \", scores['test_auc'].mean(), \"Standard Deviation: \", scores['test_auc'].std())\n",
    "print(\"Average precision: \", scores['test_pre'].mean(), \"Standard Deviation: \", scores['test_pre'].std())\n",
    "print(\"Average recall: \", scores['test_rec'].mean(), \"Standard Deviation: \", scores['test_rec'].std())\n",
    "print(\"Average MCC: \", scores['test_mcc'].mean(), \"Standard Deviation: \", scores['test_mcc'].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "TCGA_clinical = pd.read_csv(r\"最终数据/tcga_clinical.csv\", index_col=0).T[WHITE].T.values\n",
    "TCGA_clinical = MinMaxScaler().fit_transform(TCGA_clinical)\n",
    "TCGA_label = pd.read_csv(r\"最终数据/tcga_label.csv\", index_col=0)[\"os_label\"].values\n",
    "TCGA_gene = pd.read_csv(r\"最终数据/final_tcga_gene.csv\", index_col=0)\n",
    "TCGA_cna = pd.read_csv(r\"最终数据/final_tcga_cna.csv\", index_col=0).T[WHITE].T.values\n",
    "\n",
    "TCGA_label = TCGA_label.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "TCGA_label_WHITE = pd.read_csv(r\"最终数据/tcga_label.csv\", index_col=0).T[WHITE].T[\"os_label\"].values\n",
    "TCGA_label_BLACK = pd.read_csv(r\"最终数据/tcga_label.csv\", index_col=0).T[BLACK].T[\"os_label\"].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A0_Samples\n",
       "TCGA-A2-A0YF    0\n",
       "TCGA-OL-A5RV    0\n",
       "TCGA-LL-A7SZ    0\n",
       "TCGA-OL-A5RW    0\n",
       "TCGA-EW-A6SD    0\n",
       "               ..\n",
       "TCGA-PL-A8LV    0\n",
       "TCGA-OL-A66L    0\n",
       "TCGA-S3-AA14    0\n",
       "TCGA-3C-AALK    0\n",
       "TCGA-A2-A4RX    0\n",
       "Name: os_label, Length: 181, dtype: object"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TCGA_label_BLACK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "skf = StratifiedKFold(n_splits=5,shuffle=True, random_state=42)\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(skf.split(TCGA_cna, TCGA_label)):\n",
    "    with open(f'BLACK_train_index_fold{i}.pkl', 'wb') as f:\n",
    "        pickle.dump(train_index, f)\n",
    "    with open(f'BLACK_test_index_fold{i}.pkl', 'wb') as f:\n",
    "        pickle.dump(test_index, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 临床信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, initializers\n",
    "from tensorflow.keras.layers import Input, Conv1D, BatchNormalization, Activation, GlobalAveragePooling1D, Dense, Dropout, Concatenate, Layer\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.activations import sigmoid\n",
    "from keras.models import Model\n",
    "from keras.layers import Conv1D, Flatten, Dense, Input\n",
    "from keras import initializers, regularizers\n",
    "from keras import layers, models, initializers, backend as K\n",
    "from keras.activations import softmax\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv1D, BatchNormalization, Activation, Concatenate, Dropout, Flatten, Dense\n",
    "from tensorflow.keras.initializers import GlorotNormal, Constant\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, initializers\n",
    "\n",
    "class ExternalAttention(layers.Layer):\n",
    "    def __init__(self, d_model=96, S=64, **kwargs):\n",
    "        super(ExternalAttention, self).__init__(**kwargs)\n",
    "        self.d_model = d_model\n",
    "        self.S = S\n",
    "        self.mk = layers.Dense(S, use_bias=False, kernel_initializer=initializers.RandomNormal(stddev=0.001))\n",
    "        self.mv = layers.Dense(self.d_model, use_bias=False, kernel_initializer=initializers.RandomNormal(stddev=0.001))\n",
    "\n",
    "    def call(self, queries):\n",
    "        attn = self.mk(queries)  # [bs, n, S]\n",
    "        attn = tf.nn.softmax(attn)  # [bs, n, S]\n",
    "        attn = attn / tf.reduce_sum(attn, axis=2, keepdims=True)  # Normalize the softmax output [bs, n, S]\n",
    "        out = self.mv(attn)  # [bs, n, d_model]\n",
    "        return out\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(ExternalAttention, self).get_config()\n",
    "        config.update({\n",
    "            'd_model': self.d_model,\n",
    "            'S': self.S\n",
    "        })\n",
    "        return config \n",
    "    \n",
    "    \n",
    "class ChannelGatingLayer(Layer):\n",
    "    def __init__(self, channels, **kwargs):\n",
    "        super(ChannelGatingLayer, self).__init__(**kwargs)\n",
    "        self.channels = channels\n",
    "        self.gamma = self.add_weight(shape=(1, 1, channels), initializer=Constant(value=1.0), trainable=True)\n",
    "        self.beta = self.add_weight(shape=(1, 1, channels), initializer=Constant(value=0.0), trainable=True)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        # Channel gating computation\n",
    "        avg = GlobalAveragePooling1D()(inputs)\n",
    "        avg = tf.expand_dims(avg, axis=1)  # Make it 3D to match with gamma and beta\n",
    "        avg = tf.add(tf.multiply(avg, self.gamma), self.beta)\n",
    "        avg = tf.sigmoid(avg)\n",
    "        \n",
    "        # Apply gating to the inputs\n",
    "        return inputs * avg\n",
    "\n",
    "def create_clincal_model(input_shape):\n",
    "    init = GlorotNormal(seed=1)\n",
    "    bias_init = Constant(value=0.1)\n",
    "    \n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    x = Conv1D(filters=10, kernel_size=2, strides=1, padding=\"same\",\n",
    "               kernel_initializer=init, bias_initializer=bias_init)(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    residual = x\n",
    "\n",
    "    xp1 = Conv1D(filters=10, kernel_size=3, strides=1, padding=\"same\", use_bias=False)(x)\n",
    "    xp1 = BatchNormalization()(xp1)\n",
    "\n",
    "    xp2 = Conv1D(filters=10, kernel_size=3, strides=1, dilation_rate=2, padding=\"same\", use_bias=False)(x)\n",
    "    xp2 = BatchNormalization()(xp2)\n",
    "\n",
    "    x = Concatenate(axis=-1)([xp1, xp2])\n",
    "    x = Dropout(0.5)(x) \n",
    " \n",
    "    # Assuming ExternalAttention is a custom layer that you've defined elsewhere\n",
    "    x = ExternalAttention(d_model=20)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.25)(x)  \n",
    "\n",
    "    x = x + Conv1D(filters=20, kernel_size=1, strides=1, use_bias=False)(residual)\n",
    "    x = BatchNormalization()(x)\n",
    "   \n",
    "    x = Flatten()(x)\n",
    "\n",
    "    x = Dense(units=300, activation=\"tanh\", kernel_regularizer=l1(0.01))(x)\n",
    "    x = Dropout(0.5)(x)  \n",
    "    outputs = Dense(units=1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on fold 1...\n",
      "Epoch 1/50\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 30.5075 - accuracy: 0.7110 - val_loss: 24.7177 - val_accuracy: 0.7297\n",
      "Epoch 2/50\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 19.9153 - accuracy: 0.8028 - val_loss: 15.5194 - val_accuracy: 0.2432\n",
      "Epoch 3/50\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 11.8216 - accuracy: 0.8119 - val_loss: 8.9713 - val_accuracy: 0.2432\n",
      "Epoch 4/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 6.4645 - accuracy: 0.8486 - val_loss: 5.1203 - val_accuracy: 0.2432\n",
      "Epoch 5/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 3.6364 - accuracy: 0.8349 - val_loss: 3.2861 - val_accuracy: 0.2432\n",
      "Epoch 6/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 2.2058 - accuracy: 0.8211 - val_loss: 2.3113 - val_accuracy: 0.2432\n",
      "Epoch 7/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1.6157 - accuracy: 0.7752 - val_loss: 1.9638 - val_accuracy: 0.2432\n",
      "Epoch 8/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1.2627 - accuracy: 0.8119 - val_loss: 1.6510 - val_accuracy: 0.2432\n",
      "Epoch 9/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1.2953 - accuracy: 0.7431 - val_loss: 1.5969 - val_accuracy: 0.3243\n",
      "Epoch 10/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1.0537 - accuracy: 0.8486 - val_loss: 1.5836 - val_accuracy: 0.2432\n",
      "Epoch 11/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1.0652 - accuracy: 0.7615 - val_loss: 1.5219 - val_accuracy: 0.2973\n",
      "Epoch 12/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.8991 - accuracy: 0.8303 - val_loss: 1.4005 - val_accuracy: 0.3514\n",
      "Epoch 13/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.8050 - accuracy: 0.8670 - val_loss: 1.2018 - val_accuracy: 0.4324\n",
      "Epoch 14/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.8989 - accuracy: 0.8211 - val_loss: 1.1892 - val_accuracy: 0.5405\n",
      "Epoch 15/50\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 0.8894 - accuracy: 0.8257 - val_loss: 1.1279 - val_accuracy: 0.5405\n",
      "Epoch 16/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.8861 - accuracy: 0.8303 - val_loss: 1.1691 - val_accuracy: 0.5676\n",
      "Epoch 17/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.8765 - accuracy: 0.8440 - val_loss: 1.0762 - val_accuracy: 0.6757\n",
      "Epoch 18/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.8569 - accuracy: 0.8257 - val_loss: 0.9809 - val_accuracy: 0.7297\n",
      "Epoch 19/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.8221 - accuracy: 0.8578 - val_loss: 1.1099 - val_accuracy: 0.6216\n",
      "Epoch 20/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.8017 - accuracy: 0.8624 - val_loss: 0.9603 - val_accuracy: 0.6757\n",
      "Epoch 21/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.8337 - accuracy: 0.8532 - val_loss: 1.0734 - val_accuracy: 0.6216\n",
      "Epoch 22/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.7906 - accuracy: 0.8624 - val_loss: 0.9920 - val_accuracy: 0.7297\n",
      "Epoch 23/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.8125 - accuracy: 0.8670 - val_loss: 0.9778 - val_accuracy: 0.7297\n",
      "Epoch 24/50\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.9364 - accuracy: 0.81 - 0s 4ms/step - loss: 0.9034 - accuracy: 0.8303 - val_loss: 1.0829 - val_accuracy: 0.7568\n",
      "Epoch 25/50\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 0.7854 - accuracy: 0.8624 - val_loss: 1.0023 - val_accuracy: 0.7568\n",
      "Epoch 26/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.7609 - accuracy: 0.8486 - val_loss: 1.0119 - val_accuracy: 0.7568\n",
      "Epoch 27/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.7963 - accuracy: 0.8670 - val_loss: 1.0750 - val_accuracy: 0.6757\n",
      "Epoch 28/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.6443 - accuracy: 0.9174 - val_loss: 0.9252 - val_accuracy: 0.7568\n",
      "Epoch 29/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.8102 - accuracy: 0.8670 - val_loss: 0.9846 - val_accuracy: 0.7568\n",
      "Epoch 30/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.8379 - accuracy: 0.8578 - val_loss: 1.0981 - val_accuracy: 0.7027\n",
      "Epoch 31/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.8338 - accuracy: 0.8440 - val_loss: 0.9524 - val_accuracy: 0.7838\n",
      "Epoch 32/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.8424 - accuracy: 0.8578 - val_loss: 0.9770 - val_accuracy: 0.7568\n",
      "Epoch 33/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.7322 - accuracy: 0.8807 - val_loss: 0.8817 - val_accuracy: 0.7568\n",
      "Epoch 34/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.8558 - accuracy: 0.8624 - val_loss: 1.0824 - val_accuracy: 0.8108\n",
      "Epoch 35/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.7984 - accuracy: 0.8532 - val_loss: 0.9699 - val_accuracy: 0.8108\n",
      "Epoch 36/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.7711 - accuracy: 0.8440 - val_loss: 0.9502 - val_accuracy: 0.7297\n",
      "Epoch 37/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.7494 - accuracy: 0.8945 - val_loss: 1.0363 - val_accuracy: 0.7297\n",
      "Epoch 38/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.7657 - accuracy: 0.8394 - val_loss: 1.0732 - val_accuracy: 0.7838\n",
      "Epoch 39/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.7994 - accuracy: 0.8807 - val_loss: 1.0888 - val_accuracy: 0.8108\n",
      "Epoch 40/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.7259 - accuracy: 0.8807 - val_loss: 0.9626 - val_accuracy: 0.8108\n",
      "Epoch 41/50\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 0.6697 - accuracy: 0.8670 - val_loss: 1.1253 - val_accuracy: 0.7027\n",
      "Epoch 42/50\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 0.6880 - accuracy: 0.8807 - val_loss: 0.9866 - val_accuracy: 0.7297\n",
      "Epoch 43/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.7742 - accuracy: 0.8670 - val_loss: 1.0439 - val_accuracy: 0.7297\n",
      "Epoch 44/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.6458 - accuracy: 0.9037 - val_loss: 1.0483 - val_accuracy: 0.7297\n",
      "Epoch 45/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.7379 - accuracy: 0.8624 - val_loss: 1.1301 - val_accuracy: 0.7027\n",
      "Epoch 46/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.6795 - accuracy: 0.9083 - val_loss: 1.0430 - val_accuracy: 0.7297\n",
      "Epoch 47/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.7012 - accuracy: 0.8670 - val_loss: 1.0829 - val_accuracy: 0.7568\n",
      "Epoch 48/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.7666 - accuracy: 0.8853 - val_loss: 1.1302 - val_accuracy: 0.7027\n",
      "Epoch 49/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.8967 - accuracy: 0.8670 - val_loss: 1.3000 - val_accuracy: 0.7027\n",
      "Epoch 50/50\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 0.7523 - accuracy: 0.8899 - val_loss: 1.1188 - val_accuracy: 0.7297\n",
      "Score fo/r fold 1 (0.8378378378378378, 0.7817460317460317, 0.8, 0.4444444444444444, 0.5129522781726773, 0.9677205)\n",
      "Training on fold 2...\n",
      "Epoch 1/50\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 30.7679 - accuracy: 0.6295 - val_loss: 25.2171 - val_accuracy: 0.2222\n",
      "Epoch 2/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 20.3446 - accuracy: 0.7946 - val_loss: 16.0759 - val_accuracy: 0.2222\n",
      "Epoch 3/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 12.3405 - accuracy: 0.7857 - val_loss: 9.4671 - val_accuracy: 0.2222\n",
      "Epoch 4/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 6.8726 - accuracy: 0.7902 - val_loss: 5.4578 - val_accuracy: 0.2222\n",
      "Epoch 5/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 3.9662 - accuracy: 0.7679 - val_loss: 3.4703 - val_accuracy: 0.2222\n",
      "Epoch 6/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 2.5343 - accuracy: 0.7902 - val_loss: 2.5197 - val_accuracy: 0.2222\n",
      "Epoch 7/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1.7818 - accuracy: 0.7857 - val_loss: 1.8824 - val_accuracy: 0.2222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1.3374 - accuracy: 0.7812 - val_loss: 1.6174 - val_accuracy: 0.2222\n",
      "Epoch 9/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1.1134 - accuracy: 0.7991 - val_loss: 1.5371 - val_accuracy: 0.2222\n",
      "Epoch 10/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1.0132 - accuracy: 0.7500 - val_loss: 1.4343 - val_accuracy: 0.2222\n",
      "Epoch 11/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.9838 - accuracy: 0.7589 - val_loss: 1.4189 - val_accuracy: 0.2222\n",
      "Epoch 12/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.9446 - accuracy: 0.7545 - val_loss: 1.1913 - val_accuracy: 0.3056\n",
      "Epoch 13/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.8832 - accuracy: 0.7902 - val_loss: 1.1551 - val_accuracy: 0.4167\n",
      "Epoch 14/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.9040 - accuracy: 0.7634 - val_loss: 1.3226 - val_accuracy: 0.2500\n",
      "Epoch 15/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.8887 - accuracy: 0.7768 - val_loss: 1.1750 - val_accuracy: 0.4167\n",
      "Epoch 16/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.9173 - accuracy: 0.7500 - val_loss: 1.0865 - val_accuracy: 0.6389\n",
      "Epoch 17/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.8656 - accuracy: 0.8080 - val_loss: 1.2638 - val_accuracy: 0.3056\n",
      "Epoch 18/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.8235 - accuracy: 0.7812 - val_loss: 1.1427 - val_accuracy: 0.3889\n",
      "Epoch 19/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.8129 - accuracy: 0.8080 - val_loss: 0.9740 - val_accuracy: 0.6667\n",
      "Epoch 20/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.8166 - accuracy: 0.7991 - val_loss: 0.9723 - val_accuracy: 0.6944\n",
      "Epoch 21/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.8237 - accuracy: 0.8036 - val_loss: 1.1280 - val_accuracy: 0.6944\n",
      "Epoch 22/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.9374 - accuracy: 0.7768 - val_loss: 0.9255 - val_accuracy: 0.7500\n",
      "Epoch 23/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.8870 - accuracy: 0.7723 - val_loss: 0.8778 - val_accuracy: 0.8056\n",
      "Epoch 24/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.8352 - accuracy: 0.8080 - val_loss: 0.9521 - val_accuracy: 0.7500\n",
      "Epoch 25/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.8421 - accuracy: 0.8125 - val_loss: 0.8846 - val_accuracy: 0.8611\n",
      "Epoch 26/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.8310 - accuracy: 0.8393 - val_loss: 0.7955 - val_accuracy: 0.8889\n",
      "Epoch 27/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.8164 - accuracy: 0.8348 - val_loss: 1.1642 - val_accuracy: 0.6111\n",
      "Epoch 28/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.9530 - accuracy: 0.7634 - val_loss: 1.2772 - val_accuracy: 0.5833\n",
      "Epoch 29/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.9448 - accuracy: 0.7902 - val_loss: 0.8813 - val_accuracy: 0.8333\n",
      "Epoch 30/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.8905 - accuracy: 0.8036 - val_loss: 0.9390 - val_accuracy: 0.7778\n",
      "Epoch 31/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.8178 - accuracy: 0.8170 - val_loss: 0.9234 - val_accuracy: 0.7222\n",
      "Epoch 32/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.8080 - accuracy: 0.8170 - val_loss: 0.7958 - val_accuracy: 0.8333\n",
      "Epoch 33/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.8174 - accuracy: 0.8170 - val_loss: 0.8381 - val_accuracy: 0.8056\n",
      "Epoch 34/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.9115 - accuracy: 0.7857 - val_loss: 0.8653 - val_accuracy: 0.8333\n",
      "Epoch 35/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.8295 - accuracy: 0.8170 - val_loss: 0.9366 - val_accuracy: 0.7500\n",
      "Epoch 36/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.8245 - accuracy: 0.8170 - val_loss: 0.9559 - val_accuracy: 0.7500\n",
      "Epoch 37/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.8143 - accuracy: 0.8170 - val_loss: 0.9140 - val_accuracy: 0.7222\n",
      "Epoch 38/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.8128 - accuracy: 0.8348 - val_loss: 0.8576 - val_accuracy: 0.7500\n",
      "Epoch 39/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.8241 - accuracy: 0.7812 - val_loss: 0.8582 - val_accuracy: 0.8333\n",
      "Epoch 40/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.8408 - accuracy: 0.7946 - val_loss: 0.8887 - val_accuracy: 0.7778\n",
      "Epoch 41/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.7921 - accuracy: 0.8170 - val_loss: 0.8138 - val_accuracy: 0.8056\n",
      "Epoch 42/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.7990 - accuracy: 0.8036 - val_loss: 0.8740 - val_accuracy: 0.8056\n",
      "Epoch 43/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.8197 - accuracy: 0.8170 - val_loss: 0.8165 - val_accuracy: 0.7778\n",
      "Epoch 44/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.7890 - accuracy: 0.8259 - val_loss: 0.7333 - val_accuracy: 0.8611\n",
      "Epoch 45/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.8116 - accuracy: 0.7723 - val_loss: 0.9200 - val_accuracy: 0.8056\n",
      "Epoch 46/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.8128 - accuracy: 0.8259 - val_loss: 0.8612 - val_accuracy: 0.8056\n",
      "Epoch 47/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.7753 - accuracy: 0.8214 - val_loss: 0.7035 - val_accuracy: 0.8333\n",
      "Epoch 48/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.7358 - accuracy: 0.8170 - val_loss: 0.7420 - val_accuracy: 0.7778\n",
      "Epoch 49/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.7572 - accuracy: 0.8348 - val_loss: 0.8021 - val_accuracy: 0.8333\n",
      "Epoch 50/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.7762 - accuracy: 0.8125 - val_loss: 0.7099 - val_accuracy: 0.8056\n",
      "Score fo/r fold 2 (0.8333333333333334, 0.8660714285714285, 0.75, 0.375, 0.4488328116984573, 0.6487031)\n",
      "Training on fold 3...\n",
      "Epoch 1/50\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 30.1737 - accuracy: 0.7354 - val_loss: 24.4360 - val_accuracy: 0.7222\n",
      "Epoch 2/50\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 19.5467 - accuracy: 0.7982 - val_loss: 15.2897 - val_accuracy: 0.2222\n",
      "Epoch 3/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 11.5063 - accuracy: 0.8027 - val_loss: 8.8348 - val_accuracy: 0.2222\n",
      "Epoch 4/50\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 6.2466 - accuracy: 0.8072 - val_loss: 5.1255 - val_accuracy: 0.2222\n",
      "Epoch 5/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 3.5422 - accuracy: 0.7668 - val_loss: 3.3163 - val_accuracy: 0.2222\n",
      "Epoch 6/50\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 2.1812 - accuracy: 0.7982 - val_loss: 2.4767 - val_accuracy: 0.2222\n",
      "Epoch 7/50\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 1.6064 - accuracy: 0.7578 - val_loss: 2.1034 - val_accuracy: 0.2222\n",
      "Epoch 8/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1.3001 - accuracy: 0.7758 - val_loss: 1.9044 - val_accuracy: 0.2222\n",
      "Epoch 9/50\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 1.1454 - accuracy: 0.7713 - val_loss: 1.6785 - val_accuracy: 0.2222\n",
      "Epoch 10/50\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 1.0165 - accuracy: 0.7758 - val_loss: 1.5696 - val_accuracy: 0.2222\n",
      "Epoch 11/50\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 0.9638 - accuracy: 0.8072 - val_loss: 1.5162 - val_accuracy: 0.2222\n",
      "Epoch 12/50\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 0.9442 - accuracy: 0.7803 - val_loss: 1.6676 - val_accuracy: 0.2222\n",
      "Epoch 13/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.8783 - accuracy: 0.7937 - val_loss: 1.8296 - val_accuracy: 0.2222\n",
      "Epoch 14/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.8866 - accuracy: 0.8072 - val_loss: 1.7197 - val_accuracy: 0.2500\n",
      "Epoch 15/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.8958 - accuracy: 0.7713 - val_loss: 1.5968 - val_accuracy: 0.2500\n",
      "Epoch 16/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.9140 - accuracy: 0.7892 - val_loss: 1.6531 - val_accuracy: 0.2500\n",
      "Epoch 17/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.8915 - accuracy: 0.8117 - val_loss: 1.4428 - val_accuracy: 0.3333\n",
      "Epoch 18/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.8655 - accuracy: 0.8161 - val_loss: 1.3804 - val_accuracy: 0.3333\n",
      "Epoch 19/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.8538 - accuracy: 0.8027 - val_loss: 1.2577 - val_accuracy: 0.4167\n",
      "Epoch 20/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.9293 - accuracy: 0.7623 - val_loss: 1.2442 - val_accuracy: 0.4722\n",
      "Epoch 21/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.8839 - accuracy: 0.7848 - val_loss: 1.1105 - val_accuracy: 0.5278\n",
      "Epoch 22/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.8441 - accuracy: 0.8072 - val_loss: 0.9574 - val_accuracy: 0.6944\n",
      "Epoch 23/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.8431 - accuracy: 0.8430 - val_loss: 0.9324 - val_accuracy: 0.7778\n",
      "Epoch 24/50\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 0.8875 - accuracy: 0.7892 - val_loss: 1.0576 - val_accuracy: 0.5833\n",
      "Epoch 25/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.8758 - accuracy: 0.7892 - val_loss: 1.1843 - val_accuracy: 0.4444\n",
      "Epoch 26/50\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 0.8907 - accuracy: 0.7937 - val_loss: 1.0427 - val_accuracy: 0.6111\n",
      "Epoch 27/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.8629 - accuracy: 0.8341 - val_loss: 0.8786 - val_accuracy: 0.8056\n",
      "Epoch 28/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.8390 - accuracy: 0.8430 - val_loss: 1.0071 - val_accuracy: 0.6389\n",
      "Epoch 29/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.8109 - accuracy: 0.8296 - val_loss: 0.8441 - val_accuracy: 0.8056\n",
      "Epoch 30/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.8053 - accuracy: 0.8117 - val_loss: 0.9005 - val_accuracy: 0.7500\n",
      "Epoch 31/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.7988 - accuracy: 0.8251 - val_loss: 0.8357 - val_accuracy: 0.8056\n",
      "Epoch 32/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.7946 - accuracy: 0.8341 - val_loss: 0.7950 - val_accuracy: 0.8333\n",
      "Epoch 33/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.8257 - accuracy: 0.8251 - val_loss: 0.9051 - val_accuracy: 0.6944\n",
      "Epoch 34/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.8511 - accuracy: 0.7982 - val_loss: 0.9027 - val_accuracy: 0.7778\n",
      "Epoch 35/50\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 0.8993 - accuracy: 0.7803 - val_loss: 0.9885 - val_accuracy: 0.6944\n",
      "Epoch 36/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.8007 - accuracy: 0.8386 - val_loss: 0.8386 - val_accuracy: 0.7778\n",
      "Epoch 37/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.7590 - accuracy: 0.8565 - val_loss: 0.7990 - val_accuracy: 0.8333\n",
      "Epoch 38/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.7978 - accuracy: 0.8430 - val_loss: 0.7011 - val_accuracy: 0.8611\n",
      "Epoch 39/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.7764 - accuracy: 0.8565 - val_loss: 0.7766 - val_accuracy: 0.8611\n",
      "Epoch 40/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.8091 - accuracy: 0.8206 - val_loss: 0.9256 - val_accuracy: 0.8056\n",
      "Epoch 41/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.8308 - accuracy: 0.8296 - val_loss: 0.8690 - val_accuracy: 0.8056\n",
      "Epoch 42/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.8191 - accuracy: 0.8296 - val_loss: 0.8527 - val_accuracy: 0.8333\n",
      "Epoch 43/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.8049 - accuracy: 0.8386 - val_loss: 0.7949 - val_accuracy: 0.8333\n",
      "Epoch 44/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.8500 - accuracy: 0.7892 - val_loss: 0.7736 - val_accuracy: 0.8333\n",
      "Epoch 45/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.7914 - accuracy: 0.8565 - val_loss: 0.7821 - val_accuracy: 0.8333\n",
      "Epoch 46/50\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 0.7566 - accuracy: 0.8296 - val_loss: 0.8022 - val_accuracy: 0.8333\n",
      "Epoch 47/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.7923 - accuracy: 0.8117 - val_loss: 0.7610 - val_accuracy: 0.8611\n",
      "Epoch 48/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.7599 - accuracy: 0.8565 - val_loss: 0.7238 - val_accuracy: 0.8889\n",
      "Epoch 49/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.7403 - accuracy: 0.8520 - val_loss: 0.7303 - val_accuracy: 0.8611\n",
      "Epoch 50/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.7656 - accuracy: 0.8475 - val_loss: 0.7727 - val_accuracy: 0.8333\n",
      "WARNING:tensorflow:5 out of the last 15 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000021F0A80CC80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Score fo/r fold 3 (0.8333333333333334, 0.8660714285714286, 0.75, 0.375, 0.4488328116984573, 0.8582666)\n",
      "Training on fold 4...\n",
      "Epoch 1/50\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 30.5208 - accuracy: 0.6816 - val_loss: 24.8659 - val_accuracy: 0.8889\n",
      "Epoch 2/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 20.0723 - accuracy: 0.7848 - val_loss: 15.8365 - val_accuracy: 0.1944\n",
      "Epoch 3/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 12.1573 - accuracy: 0.7982 - val_loss: 9.3257 - val_accuracy: 0.2222\n",
      "Epoch 4/50\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 6.8275 - accuracy: 0.8072 - val_loss: 5.4728 - val_accuracy: 0.2222\n",
      "Epoch 5/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 3.8980 - accuracy: 0.8027 - val_loss: 3.5082 - val_accuracy: 0.2222\n",
      "Epoch 6/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 2.4321 - accuracy: 0.8027 - val_loss: 2.4467 - val_accuracy: 0.2222\n",
      "Epoch 7/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1.6961 - accuracy: 0.8072 - val_loss: 2.0742 - val_accuracy: 0.2222\n",
      "Epoch 8/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1.2459 - accuracy: 0.8520 - val_loss: 1.5027 - val_accuracy: 0.2222\n",
      "Epoch 9/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1.1292 - accuracy: 0.8027 - val_loss: 1.4821 - val_accuracy: 0.2222\n",
      "Epoch 10/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1.0179 - accuracy: 0.7803 - val_loss: 1.5534 - val_accuracy: 0.2222\n",
      "Epoch 11/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.9634 - accuracy: 0.8072 - val_loss: 1.4532 - val_accuracy: 0.2222\n",
      "Epoch 12/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1.0337 - accuracy: 0.7668 - val_loss: 1.4222 - val_accuracy: 0.2778\n",
      "Epoch 13/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1.0637 - accuracy: 0.7668 - val_loss: 1.3773 - val_accuracy: 0.3056\n",
      "Epoch 14/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1.0116 - accuracy: 0.7803 - val_loss: 1.3862 - val_accuracy: 0.2778\n",
      "Epoch 15/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.9790 - accuracy: 0.8027 - val_loss: 1.2326 - val_accuracy: 0.5000\n",
      "Epoch 16/50\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 0.9466 - accuracy: 0.8027 - val_loss: 1.3867 - val_accuracy: 0.2778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.9268 - accuracy: 0.7937 - val_loss: 1.1948 - val_accuracy: 0.5278\n",
      "Epoch 18/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.9609 - accuracy: 0.7937 - val_loss: 1.4047 - val_accuracy: 0.3056\n",
      "Epoch 19/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.9065 - accuracy: 0.7982 - val_loss: 1.3532 - val_accuracy: 0.3333\n",
      "Epoch 20/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.8912 - accuracy: 0.7758 - val_loss: 1.0958 - val_accuracy: 0.4722\n",
      "Epoch 21/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.8920 - accuracy: 0.8072 - val_loss: 0.9840 - val_accuracy: 0.8056\n",
      "Epoch 22/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.9656 - accuracy: 0.7758 - val_loss: 1.1620 - val_accuracy: 0.6389\n",
      "Epoch 23/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.9781 - accuracy: 0.7803 - val_loss: 1.1033 - val_accuracy: 0.6944\n",
      "Epoch 24/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.9952 - accuracy: 0.7803 - val_loss: 1.0156 - val_accuracy: 0.7500\n",
      "Epoch 25/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.8915 - accuracy: 0.7937 - val_loss: 1.2051 - val_accuracy: 0.4722\n",
      "Epoch 26/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.8955 - accuracy: 0.8027 - val_loss: 1.0495 - val_accuracy: 0.5833\n",
      "Epoch 27/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.8978 - accuracy: 0.8117 - val_loss: 1.1109 - val_accuracy: 0.5556\n",
      "Epoch 28/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.8354 - accuracy: 0.8430 - val_loss: 0.8806 - val_accuracy: 0.8611\n",
      "Epoch 29/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.8381 - accuracy: 0.8251 - val_loss: 1.0949 - val_accuracy: 0.6667\n",
      "Epoch 30/50\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 0.9150 - accuracy: 0.8161 - val_loss: 1.0633 - val_accuracy: 0.6389\n",
      "Epoch 31/50\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.8892 - accuracy: 0.7848 - val_loss: 0.9464 - val_accuracy: 0.8056\n",
      "Epoch 32/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.9609 - accuracy: 0.7937 - val_loss: 0.9521 - val_accuracy: 0.8889\n",
      "Epoch 33/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.8566 - accuracy: 0.8386 - val_loss: 1.1249 - val_accuracy: 0.6111\n",
      "Epoch 34/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.8154 - accuracy: 0.8386 - val_loss: 0.9300 - val_accuracy: 0.7778\n",
      "Epoch 35/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.8208 - accuracy: 0.8296 - val_loss: 0.9028 - val_accuracy: 0.8333\n",
      "Epoch 36/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.8295 - accuracy: 0.8296 - val_loss: 1.0258 - val_accuracy: 0.6944\n",
      "Epoch 37/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.8477 - accuracy: 0.7892 - val_loss: 1.0167 - val_accuracy: 0.7500\n",
      "Epoch 38/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.7974 - accuracy: 0.8655 - val_loss: 0.9483 - val_accuracy: 0.7222\n",
      "Epoch 39/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.8010 - accuracy: 0.8251 - val_loss: 0.8228 - val_accuracy: 0.8889\n",
      "Epoch 40/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.9043 - accuracy: 0.7892 - val_loss: 1.0144 - val_accuracy: 0.7500\n",
      "Epoch 41/50\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 0.9026 - accuracy: 0.8161 - val_loss: 1.0128 - val_accuracy: 0.7222\n",
      "Epoch 42/50\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 0.8204 - accuracy: 0.8296 - val_loss: 0.8062 - val_accuracy: 0.8611\n",
      "Epoch 43/50\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 0.7782 - accuracy: 0.8341 - val_loss: 0.7516 - val_accuracy: 0.9444\n",
      "Epoch 44/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.8100 - accuracy: 0.8161 - val_loss: 0.8000 - val_accuracy: 0.8889\n",
      "Epoch 45/50\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.7832 - accuracy: 0.85 - 0s 4ms/step - loss: 0.7599 - accuracy: 0.8610 - val_loss: 0.7433 - val_accuracy: 0.8611\n",
      "Epoch 46/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.7517 - accuracy: 0.8520 - val_loss: 0.6836 - val_accuracy: 0.9167\n",
      "Epoch 47/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.7996 - accuracy: 0.8117 - val_loss: 0.7351 - val_accuracy: 0.9444\n",
      "Epoch 48/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.7602 - accuracy: 0.8430 - val_loss: 0.7761 - val_accuracy: 0.8889\n",
      "Epoch 49/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.7495 - accuracy: 0.8341 - val_loss: 0.8268 - val_accuracy: 0.8333\n",
      "Epoch 50/50\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 0.7360 - accuracy: 0.8386 - val_loss: 0.7325 - val_accuracy: 0.9444\n",
      "WARNING:tensorflow:5 out of the last 12 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000021F10740268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Score fo/r fold 4 (0.7777777777777778, 0.8303571428571429, 0.0, 0.0, 0.0, 1.9641263)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "E:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\_classification.py:873: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on fold 5...\n",
      "Epoch 1/50\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 30.2899 - accuracy: 0.7368 - val_loss: 24.3364 - val_accuracy: 0.6667\n",
      "Epoch 2/50\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 19.4931 - accuracy: 0.7763 - val_loss: 15.0764 - val_accuracy: 0.2500\n",
      "Epoch 3/50\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 11.4251 - accuracy: 0.7851 - val_loss: 8.6939 - val_accuracy: 0.2500\n",
      "Epoch 4/50\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 6.2361 - accuracy: 0.8026 - val_loss: 5.0494 - val_accuracy: 0.2500\n",
      "Epoch 5/50\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 3.5100 - accuracy: 0.7895 - val_loss: 3.1682 - val_accuracy: 0.2500\n",
      "Epoch 6/50\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 2.1973 - accuracy: 0.8026 - val_loss: 2.1972 - val_accuracy: 0.2500\n",
      "Epoch 7/50\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 1.5166 - accuracy: 0.7895 - val_loss: 1.7316 - val_accuracy: 0.2500\n",
      "Epoch 8/50\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 1.1833 - accuracy: 0.7675 - val_loss: 1.5630 - val_accuracy: 0.2500\n",
      "Epoch 9/50\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 1.0451 - accuracy: 0.8114 - val_loss: 1.4137 - val_accuracy: 0.2500\n",
      "Epoch 10/50\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.9877 - accuracy: 0.7939 - val_loss: 1.4213 - val_accuracy: 0.2500\n",
      "Epoch 11/50\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 1.0553 - accuracy: 0.7807 - val_loss: 1.4108 - val_accuracy: 0.2778\n",
      "Epoch 12/50\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.9772 - accuracy: 0.7851 - val_loss: 1.5519 - val_accuracy: 0.2778\n",
      "Epoch 13/50\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.9193 - accuracy: 0.7895 - val_loss: 1.1980 - val_accuracy: 0.3611\n",
      "Epoch 14/50\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.9223 - accuracy: 0.7895 - val_loss: 1.3099 - val_accuracy: 0.3056\n",
      "Epoch 15/50\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.8919 - accuracy: 0.7719 - val_loss: 1.0853 - val_accuracy: 0.6667\n",
      "Epoch 16/50\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.8741 - accuracy: 0.7982 - val_loss: 1.2122 - val_accuracy: 0.4167\n",
      "Epoch 17/50\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.8536 - accuracy: 0.8202 - val_loss: 1.0360 - val_accuracy: 0.6667\n",
      "Epoch 18/50\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.8745 - accuracy: 0.8246 - val_loss: 1.1455 - val_accuracy: 0.6944\n",
      "Epoch 19/50\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.8903 - accuracy: 0.8026 - val_loss: 0.8807 - val_accuracy: 0.8056\n",
      "Epoch 20/50\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.8588 - accuracy: 0.8246 - val_loss: 0.8892 - val_accuracy: 0.7778\n",
      "Epoch 21/50\n",
      "29/29 [==============================] - ETA: 0s - loss: 0.8211 - accuracy: 0.82 - 0s 4ms/step - loss: 0.8342 - accuracy: 0.8158 - val_loss: 1.0688 - val_accuracy: 0.6667\n",
      "Epoch 22/50\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.8618 - accuracy: 0.8202 - val_loss: 1.1729 - val_accuracy: 0.6111\n",
      "Epoch 23/50\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.9494 - accuracy: 0.8289 - val_loss: 0.9476 - val_accuracy: 0.8056\n",
      "Epoch 24/50\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.8341 - accuracy: 0.8465 - val_loss: 0.8520 - val_accuracy: 0.7778\n",
      "Epoch 25/50\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.8512 - accuracy: 0.8246 - val_loss: 0.9372 - val_accuracy: 0.8056\n",
      "Epoch 26/50\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.8359 - accuracy: 0.8289 - val_loss: 0.9002 - val_accuracy: 0.8056\n",
      "Epoch 27/50\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.8278 - accuracy: 0.8246 - val_loss: 0.8146 - val_accuracy: 0.7778\n",
      "Epoch 28/50\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 0.7620 - accuracy: 0.8333 - val_loss: 0.8373 - val_accuracy: 0.7778\n",
      "Epoch 29/50\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.7236 - accuracy: 0.8816 - val_loss: 0.8049 - val_accuracy: 0.7778\n",
      "Epoch 30/50\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 0.7212 - accuracy: 0.8333 - val_loss: 0.7885 - val_accuracy: 0.8056\n",
      "Epoch 31/50\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 0.7731 - accuracy: 0.8289 - val_loss: 0.8085 - val_accuracy: 0.7778\n",
      "Epoch 32/50\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.8052 - accuracy: 0.8377 - val_loss: 0.8555 - val_accuracy: 0.7778\n",
      "Epoch 33/50\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.7858 - accuracy: 0.8377 - val_loss: 0.8668 - val_accuracy: 0.7500\n",
      "Epoch 34/50\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.7479 - accuracy: 0.8509 - val_loss: 0.9223 - val_accuracy: 0.7778\n",
      "Epoch 35/50\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.7619 - accuracy: 0.8465 - val_loss: 0.9645 - val_accuracy: 0.7778\n",
      "Epoch 36/50\n",
      "29/29 [==============================] - ETA: 0s - loss: 0.8642 - accuracy: 0.77 - 0s 4ms/step - loss: 0.8511 - accuracy: 0.7982 - val_loss: 1.0803 - val_accuracy: 0.7222\n",
      "Epoch 37/50\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.7621 - accuracy: 0.8509 - val_loss: 0.7932 - val_accuracy: 0.8056\n",
      "Epoch 38/50\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.7471 - accuracy: 0.8640 - val_loss: 0.8233 - val_accuracy: 0.8056\n",
      "Epoch 39/50\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.7687 - accuracy: 0.8377 - val_loss: 0.8749 - val_accuracy: 0.8333\n",
      "Epoch 40/50\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.7736 - accuracy: 0.8509 - val_loss: 0.9315 - val_accuracy: 0.7500\n",
      "Epoch 41/50\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.7460 - accuracy: 0.8421 - val_loss: 0.9435 - val_accuracy: 0.7500\n",
      "Epoch 42/50\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 0.6865 - accuracy: 0.8596 - val_loss: 0.8298 - val_accuracy: 0.7778\n",
      "Epoch 43/50\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.7234 - accuracy: 0.8553 - val_loss: 0.8616 - val_accuracy: 0.7778\n",
      "Epoch 44/50\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.6765 - accuracy: 0.8816 - val_loss: 0.8856 - val_accuracy: 0.7778\n",
      "Epoch 45/50\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.6753 - accuracy: 0.8684 - val_loss: 0.9028 - val_accuracy: 0.7778\n",
      "Epoch 46/50\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.7448 - accuracy: 0.8553 - val_loss: 0.9536 - val_accuracy: 0.8056\n",
      "Epoch 47/50\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.7770 - accuracy: 0.8377 - val_loss: 0.9386 - val_accuracy: 0.8333\n",
      "Epoch 48/50\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.8357 - accuracy: 0.8289 - val_loss: 0.9104 - val_accuracy: 0.7778\n",
      "Epoch 49/50\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.7395 - accuracy: 0.8640 - val_loss: 0.9165 - val_accuracy: 0.8056\n",
      "Epoch 50/50\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.7500 - accuracy: 0.8509 - val_loss: 0.9698 - val_accuracy: 0.7778\n",
      "WARNING:tensorflow:6 out of the last 14 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000021F315F7B70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Score fo/r fold 5 (0.75, 0.8641975308641976, 0.0, 0.0, 0.0, 1.9980696)\n",
      "(0.8397790055248618, 0.8218568002740665, 0.7407407407407407, 0.47619047619047616, 0.5045762868447957, 0.8582666)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "E:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\_classification.py:873: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8679245283018868,\n",
       " 0.8932682963295209,\n",
       " 0.810126582278481,\n",
       " 0.6530612244897959,\n",
       " 0.644132086346931,\n",
       " 0.7824894)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "fold_no = 1\n",
    "scores = []\n",
    "all_y_true = []\n",
    "all_y_prob = []\n",
    "skf = StratifiedKFold(n_splits=5,shuffle=True, random_state=42)\n",
    "all_fold_histories = []\n",
    "\n",
    "\n",
    "for i in range(5):  \n",
    "    with open(f'BLACK_train_index_fold{i}.pkl', 'rb') as f:\n",
    "        train_index = pickle.load(f)\n",
    "    with open(f'BLACK_test_index_fold{i}.pkl', 'rb') as f:\n",
    "        test_index = pickle.load(f)\n",
    "    \n",
    "    X_train, X_test = TCGA_clinical[train_index], TCGA_clinical[test_index]\n",
    "    y_train, y_test = TCGA_label[train_index], TCGA_label[test_index]\n",
    "    \n",
    "    smote_enn = ADASYN(random_state=42)\n",
    "\n",
    "    X_train, y_train = smote_enn.fit_resample(X_train, y_train)\n",
    "    \n",
    "    model = create_clincal_model(input_shape = (11, 1))\n",
    "    print(f'Training on fold {fold_no}...')\n",
    "    history = model.fit(X_train, y_train,validation_data=(X_test,y_test), epochs=50, batch_size=8, verbose=1)\n",
    "    all_fold_histories.append(history.history)\n",
    "    # 评估模型\n",
    "    scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "    y_pred_prob = model.predict(X_test).reshape(-1)\n",
    "    all_y_true.extend(y_test)\n",
    "    all_y_prob.extend(y_pred_prob)\n",
    "    metrics = calculate_evaluation_metrics(y_test, y_pred_prob, 0.95)\n",
    "    print('Score fo/r fold',fold_no,metrics)\n",
    "    fold_no += 1\n",
    "print(calculate_evaluation_metrics(all_y_true, all_y_prob, 0.95))\n",
    "pd.DataFrame([all_y_true,all_y_prob]).T.to_csv(\"BLACK_clincal.csv\")\n",
    "# wu(0.8811513463324049, 0.8787080689909397, 0.7980295566502463, 0.6506024096385542, 0.6479587505455906, 0.53422964)\n",
    "# (0.8644382544103992, 0.887356188037173, 0.7783783783783784, 0.5783132530120482, 0.5910646621570037, 0.89936125)\n",
    "\n",
    "\n",
    "(0.8679245283018868, 0.8932682963295209, 0.810126582278481, 0.6530612244897959, 0.644132086346931, 0.7824894)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 拷贝数变异"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cna_model(input_shape):\n",
    "    init = GlorotNormal(seed=1)\n",
    "    bias_init = Constant(value=0.1)\n",
    "    \n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    x = Conv1D(filters=10, kernel_size=2, strides=1, padding=\"same\",\n",
    "               kernel_initializer=init, bias_initializer=bias_init)(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    residual = x\n",
    "\n",
    "    xp1 = Conv1D(filters=10, kernel_size=3, strides=1, padding=\"same\", use_bias=False)(x)\n",
    "    xp1 = BatchNormalization()(xp1)\n",
    "\n",
    "    xp2 = Conv1D(filters=10, kernel_size=3, strides=1, dilation_rate=2, padding=\"same\", use_bias=False)(x)\n",
    "    xp2 = BatchNormalization()(xp2)\n",
    "\n",
    "    x = Concatenate(axis=-1)([xp1, xp2])\n",
    "    x = Dropout(0.5)(x) \n",
    " \n",
    "    # Assuming ExternalAttention is a custom layer that you've defined elsewhere\n",
    "    x = ExternalAttention(d_model=20)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.25)(x)  \n",
    "\n",
    "    x = x + Conv1D(filters=20, kernel_size=1, strides=1, use_bias=False)(residual)\n",
    "    x = BatchNormalization()(x)\n",
    "   \n",
    "    x = Flatten()(x)\n",
    "\n",
    "    x = Dense(units=300, activation=\"tanh\", kernel_regularizer=l1(0.01))(x)\n",
    "    x = Dropout(0.5)(x)  \n",
    "    outputs = Dense(units=1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on fold 1...\n",
      "Epoch 1/50\n",
      "14/14 [==============================] - 1s 55ms/step - loss: 192.6658 - accuracy: 0.8009 - val_loss: 115.4292 - val_accuracy: 0.7297\n",
      "Epoch 2/50\n",
      "14/14 [==============================] - 1s 38ms/step - loss: 71.4169 - accuracy: 0.9548 - val_loss: 33.9859 - val_accuracy: 0.7297\n",
      "Epoch 3/50\n",
      "14/14 [==============================] - 1s 40ms/step - loss: 22.6089 - accuracy: 0.9502 - val_loss: 14.9085 - val_accuracy: 0.7568\n",
      "Epoch 4/50\n",
      "14/14 [==============================] - 1s 36ms/step - loss: 11.6858 - accuracy: 0.8145 - val_loss: 9.8939 - val_accuracy: 0.7568\n",
      "Epoch 5/50\n",
      "14/14 [==============================] - 1s 36ms/step - loss: 8.2925 - accuracy: 0.8688 - val_loss: 7.4183 - val_accuracy: 0.6486\n",
      "Epoch 6/50\n",
      "14/14 [==============================] - 1s 37ms/step - loss: 6.5688 - accuracy: 0.8416 - val_loss: 6.9460 - val_accuracy: 0.7568\n",
      "Epoch 7/50\n",
      "14/14 [==============================] - 1s 37ms/step - loss: 6.1752 - accuracy: 0.8824 - val_loss: 6.0171 - val_accuracy: 0.7027\n",
      "Epoch 8/50\n",
      "14/14 [==============================] - 1s 37ms/step - loss: 6.0483 - accuracy: 0.8145 - val_loss: 6.9296 - val_accuracy: 0.7568\n",
      "Epoch 9/50\n",
      "14/14 [==============================] - 1s 39ms/step - loss: 6.2048 - accuracy: 0.8416 - val_loss: 6.3598 - val_accuracy: 0.7838\n",
      "Epoch 10/50\n",
      "14/14 [==============================] - 1s 37ms/step - loss: 5.8944 - accuracy: 0.8462 - val_loss: 5.8845 - val_accuracy: 0.5676\n",
      "Epoch 11/50\n",
      "14/14 [==============================] - 1s 37ms/step - loss: 5.6381 - accuracy: 0.8235 - val_loss: 5.9610 - val_accuracy: 0.7297\n",
      "Epoch 12/50\n",
      "14/14 [==============================] - 1s 38ms/step - loss: 5.2130 - accuracy: 0.8733 - val_loss: 5.3298 - val_accuracy: 0.7297\n",
      "Epoch 13/50\n",
      "14/14 [==============================] - 1s 37ms/step - loss: 4.8121 - accuracy: 0.8914 - val_loss: 4.9897 - val_accuracy: 0.6216\n",
      "Epoch 14/50\n",
      "14/14 [==============================] - 0s 35ms/step - loss: 4.6846 - accuracy: 0.8824 - val_loss: 4.9116 - val_accuracy: 0.8108\n",
      "Epoch 15/50\n",
      "14/14 [==============================] - 1s 38ms/step - loss: 4.3614 - accuracy: 0.8914 - val_loss: 4.8857 - val_accuracy: 0.4324\n",
      "Epoch 16/50\n",
      "14/14 [==============================] - 0s 35ms/step - loss: 4.4361 - accuracy: 0.9050 - val_loss: 4.8607 - val_accuracy: 0.5676\n",
      "Epoch 17/50\n",
      "14/14 [==============================] - 1s 37ms/step - loss: 4.5913 - accuracy: 0.8416 - val_loss: 4.9913 - val_accuracy: 0.7027\n",
      "Epoch 18/50\n",
      "14/14 [==============================] - 0s 35ms/step - loss: 4.7365 - accuracy: 0.8597 - val_loss: 4.9280 - val_accuracy: 0.7568\n",
      "Epoch 19/50\n",
      "14/14 [==============================] - 1s 36ms/step - loss: 4.3588 - accuracy: 0.8959 - val_loss: 4.6342 - val_accuracy: 0.5946\n",
      "Epoch 20/50\n",
      "14/14 [==============================] - 0s 35ms/step - loss: 4.2790 - accuracy: 0.8959 - val_loss: 4.6704 - val_accuracy: 0.7297\n",
      "Epoch 21/50\n",
      "14/14 [==============================] - 1s 36ms/step - loss: 4.1225 - accuracy: 0.9231 - val_loss: 4.2979 - val_accuracy: 0.7568\n",
      "Epoch 22/50\n",
      "14/14 [==============================] - 1s 37ms/step - loss: 4.2485 - accuracy: 0.8914 - val_loss: 4.5102 - val_accuracy: 0.7568\n",
      "Epoch 23/50\n",
      "14/14 [==============================] - 1s 37ms/step - loss: 4.2390 - accuracy: 0.8824 - val_loss: 4.6401 - val_accuracy: 0.5946\n",
      "Epoch 24/50\n",
      "14/14 [==============================] - 1s 36ms/step - loss: 4.2034 - accuracy: 0.9050 - val_loss: 4.5269 - val_accuracy: 0.7297\n",
      "Epoch 25/50\n",
      "14/14 [==============================] - 0s 35ms/step - loss: 4.1819 - accuracy: 0.9005 - val_loss: 4.6015 - val_accuracy: 0.6486\n",
      "Epoch 26/50\n",
      "14/14 [==============================] - 0s 35ms/step - loss: 4.2524 - accuracy: 0.9186 - val_loss: 4.3767 - val_accuracy: 0.7838\n",
      "Epoch 27/50\n",
      "14/14 [==============================] - 1s 39ms/step - loss: 4.0193 - accuracy: 0.9321 - val_loss: 4.1115 - val_accuracy: 0.7838\n",
      "Epoch 28/50\n",
      "14/14 [==============================] - 1s 38ms/step - loss: 3.9587 - accuracy: 0.9005 - val_loss: 4.3819 - val_accuracy: 0.7568\n",
      "Epoch 29/50\n",
      "14/14 [==============================] - 1s 37ms/step - loss: 4.0568 - accuracy: 0.8733 - val_loss: 4.4948 - val_accuracy: 0.8108\n",
      "Epoch 30/50\n",
      "14/14 [==============================] - 1s 40ms/step - loss: 4.0114 - accuracy: 0.8959 - val_loss: 4.3022 - val_accuracy: 0.7838\n",
      "Epoch 31/50\n",
      "14/14 [==============================] - 1s 44ms/step - loss: 4.0439 - accuracy: 0.9095 - val_loss: 4.1622 - val_accuracy: 0.7838\n",
      "Epoch 32/50\n",
      "14/14 [==============================] - 1s 40ms/step - loss: 3.8624 - accuracy: 0.8824 - val_loss: 4.3644 - val_accuracy: 0.8108\n",
      "Epoch 33/50\n",
      "14/14 [==============================] - 1s 41ms/step - loss: 3.9224 - accuracy: 0.9095 - val_loss: 4.1320 - val_accuracy: 0.8108\n",
      "Epoch 34/50\n",
      "14/14 [==============================] - 1s 41ms/step - loss: 3.7047 - accuracy: 0.9367 - val_loss: 3.9672 - val_accuracy: 0.6757\n",
      "Epoch 35/50\n",
      "14/14 [==============================] - 1s 38ms/step - loss: 3.5378 - accuracy: 0.9276 - val_loss: 4.0119 - val_accuracy: 0.7838\n",
      "Epoch 36/50\n",
      "14/14 [==============================] - 1s 42ms/step - loss: 4.0176 - accuracy: 0.8914 - val_loss: 4.4806 - val_accuracy: 0.7568\n",
      "Epoch 37/50\n",
      "14/14 [==============================] - 1s 40ms/step - loss: 3.7978 - accuracy: 0.9140 - val_loss: 4.0401 - val_accuracy: 0.8108\n",
      "Epoch 38/50\n",
      "14/14 [==============================] - 1s 39ms/step - loss: 4.1124 - accuracy: 0.8959 - val_loss: 4.3461 - val_accuracy: 0.8108\n",
      "Epoch 39/50\n",
      "14/14 [==============================] - 1s 43ms/step - loss: 4.0073 - accuracy: 0.9321 - val_loss: 4.1610 - val_accuracy: 0.7838\n",
      "Epoch 40/50\n",
      "14/14 [==============================] - 1s 41ms/step - loss: 3.7531 - accuracy: 0.9186 - val_loss: 3.9320 - val_accuracy: 0.8108\n",
      "Epoch 41/50\n",
      "14/14 [==============================] - 1s 39ms/step - loss: 3.5593 - accuracy: 0.9367 - val_loss: 4.0434 - val_accuracy: 0.7568\n",
      "Epoch 42/50\n",
      "14/14 [==============================] - 1s 41ms/step - loss: 3.9792 - accuracy: 0.8688 - val_loss: 4.5944 - val_accuracy: 0.6757\n",
      "Epoch 43/50\n",
      "14/14 [==============================] - 1s 40ms/step - loss: 4.0080 - accuracy: 0.9005 - val_loss: 4.2465 - val_accuracy: 0.6757\n",
      "Epoch 44/50\n",
      "14/14 [==============================] - 1s 40ms/step - loss: 3.6176 - accuracy: 0.9548 - val_loss: 3.8306 - val_accuracy: 0.7838\n",
      "Epoch 45/50\n",
      "14/14 [==============================] - 1s 38ms/step - loss: 3.3918 - accuracy: 0.9548 - val_loss: 3.7039 - val_accuracy: 0.7838\n",
      "Epoch 46/50\n",
      "14/14 [==============================] - 1s 42ms/step - loss: 3.4512 - accuracy: 0.9276 - val_loss: 3.8751 - val_accuracy: 0.8378\n",
      "Epoch 47/50\n",
      "14/14 [==============================] - 1s 39ms/step - loss: 3.3986 - accuracy: 0.9367 - val_loss: 3.7457 - val_accuracy: 0.7838\n",
      "Epoch 48/50\n",
      "14/14 [==============================] - 1s 36ms/step - loss: 3.2659 - accuracy: 0.9502 - val_loss: 3.6353 - val_accuracy: 0.7838\n",
      "Epoch 49/50\n",
      "14/14 [==============================] - 0s 34ms/step - loss: 3.2652 - accuracy: 0.9548 - val_loss: 3.7638 - val_accuracy: 0.7568\n",
      "Epoch 50/50\n",
      "14/14 [==============================] - 0s 35ms/step - loss: 3.5282 - accuracy: 0.9502 - val_loss: 4.0434 - val_accuracy: 0.7027\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000021F4A866AE8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Score fo/r fold 1 (0.7837837837837838, 0.6626984126984128, 0.6, 0.3333333333333333, 0.3286878675669583, 0.73024064)\n",
      "Training on fold 2...\n",
      "Epoch 1/50\n",
      "15/15 [==============================] - 1s 59ms/step - loss: 187.7676 - accuracy: 0.8622 - val_loss: 102.5003 - val_accuracy: 0.7778\n",
      "Epoch 2/50\n",
      "15/15 [==============================] - 1s 39ms/step - loss: 61.0001 - accuracy: 0.9200 - val_loss: 27.1193 - val_accuracy: 0.7778\n",
      "Epoch 3/50\n",
      "15/15 [==============================] - 1s 37ms/step - loss: 21.3404 - accuracy: 0.8889 - val_loss: 14.3565 - val_accuracy: 0.8056\n",
      "Epoch 4/50\n",
      "15/15 [==============================] - 1s 38ms/step - loss: 11.0954 - accuracy: 0.8489 - val_loss: 9.5970 - val_accuracy: 0.5833\n",
      "Epoch 5/50\n",
      "15/15 [==============================] - 1s 39ms/step - loss: 7.9685 - accuracy: 0.8400 - val_loss: 7.8206 - val_accuracy: 0.5278\n",
      "Epoch 6/50\n",
      "15/15 [==============================] - 1s 38ms/step - loss: 9.6349 - accuracy: 0.8089 - val_loss: 9.7531 - val_accuracy: 0.5556\n",
      "Epoch 7/50\n",
      "15/15 [==============================] - 1s 38ms/step - loss: 8.3532 - accuracy: 0.8044 - val_loss: 7.9500 - val_accuracy: 0.5556\n",
      "Epoch 8/50\n",
      "15/15 [==============================] - 1s 37ms/step - loss: 8.5610 - accuracy: 0.8578 - val_loss: 8.4747 - val_accuracy: 0.6944\n",
      "Epoch 9/50\n",
      "15/15 [==============================] - 1s 41ms/step - loss: 6.7249 - accuracy: 0.8978 - val_loss: 5.9816 - val_accuracy: 0.5833\n",
      "Epoch 10/50\n",
      "15/15 [==============================] - 1s 40ms/step - loss: 5.0929 - accuracy: 0.9111 - val_loss: 5.5158 - val_accuracy: 0.7500\n",
      "Epoch 11/50\n",
      "15/15 [==============================] - 1s 38ms/step - loss: 6.6090 - accuracy: 0.8667 - val_loss: 7.0822 - val_accuracy: 0.3611\n",
      "Epoch 12/50\n",
      "15/15 [==============================] - 1s 38ms/step - loss: 9.7535 - accuracy: 0.7956 - val_loss: 9.5262 - val_accuracy: 0.5278\n",
      "Epoch 13/50\n",
      "15/15 [==============================] - 1s 38ms/step - loss: 7.2854 - accuracy: 0.8844 - val_loss: 6.4617 - val_accuracy: 0.3889\n",
      "Epoch 14/50\n",
      "15/15 [==============================] - 1s 38ms/step - loss: 6.6048 - accuracy: 0.8356 - val_loss: 6.1085 - val_accuracy: 0.5556\n",
      "Epoch 15/50\n",
      "15/15 [==============================] - 1s 38ms/step - loss: 4.7943 - accuracy: 0.9022 - val_loss: 4.6172 - val_accuracy: 0.6944\n",
      "Epoch 16/50\n",
      "15/15 [==============================] - 1s 39ms/step - loss: 4.2666 - accuracy: 0.8933 - val_loss: 4.8441 - val_accuracy: 0.6389\n",
      "Epoch 17/50\n",
      "15/15 [==============================] - 1s 42ms/step - loss: 4.9299 - accuracy: 0.8667 - val_loss: 4.7918 - val_accuracy: 0.6667\n",
      "Epoch 18/50\n",
      "15/15 [==============================] - 1s 40ms/step - loss: 4.3059 - accuracy: 0.8489 - val_loss: 4.9559 - val_accuracy: 0.6667\n",
      "Epoch 19/50\n",
      "15/15 [==============================] - 1s 40ms/step - loss: 5.0381 - accuracy: 0.8667 - val_loss: 5.3647 - val_accuracy: 0.6111\n",
      "Epoch 20/50\n",
      "15/15 [==============================] - 1s 42ms/step - loss: 4.7829 - accuracy: 0.8889 - val_loss: 4.8669 - val_accuracy: 0.6389\n",
      "Epoch 21/50\n",
      "15/15 [==============================] - 1s 42ms/step - loss: 4.5071 - accuracy: 0.8756 - val_loss: 4.7216 - val_accuracy: 0.7222\n",
      "Epoch 22/50\n",
      "15/15 [==============================] - 1s 40ms/step - loss: 4.0740 - accuracy: 0.9111 - val_loss: 4.5336 - val_accuracy: 0.7500\n",
      "Epoch 23/50\n",
      "15/15 [==============================] - 1s 39ms/step - loss: 4.6967 - accuracy: 0.8578 - val_loss: 4.9932 - val_accuracy: 0.6389\n",
      "Epoch 24/50\n",
      " 1/15 [=>............................] - ETA: 0s - loss: 4.4531 - accuracy: 1.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-94-3f96fe909690>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_cna_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m280\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Training on fold {fold_no}...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;31m# 评估模型\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    805\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 807\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[0;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1924\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "fold_no = 1\n",
    "scores = []\n",
    "all_y_true = []\n",
    "all_y_prob = []\n",
    "skf = StratifiedKFold(n_splits=5,shuffle=True, random_state=42)\n",
    "all_fold_histories = []\n",
    "\n",
    "for i in range(5):  \n",
    "    with open(f'BLACK_train_index_fold{i}.pkl', 'rb') as f:\n",
    "        train_index = pickle.load(f)\n",
    "    with open(f'BLACK_test_index_fold{i}.pkl', 'rb') as f:\n",
    "        test_index = pickle.load(f)\n",
    "        \n",
    "    X_train, X_test = TCGA_cna[train_index], TCGA_cna[test_index]\n",
    "    y_train, y_test = TCGA_label[train_index], TCGA_label[test_index]\n",
    "    adasyn = ADASYN(random_state=42)\n",
    "    X_train, y_train = adasyn.fit_resample(X_train, y_train)\n",
    "    \n",
    "    model = create_cna_model(input_shape = (280, 1))\n",
    "    print(f'Training on fold {fold_no}...')\n",
    "    history = model.fit(X_train, y_train,validation_data=(X_test,y_test), epochs=50, batch_size=16, verbose=1)\n",
    "    \n",
    "    # 评估模型\n",
    "    all_fold_histories.append(history.history)\n",
    "    scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "    y_pred_prob = model.predict(X_test).reshape(-1)\n",
    "    all_y_true.extend(y_test)\n",
    "    all_y_prob.extend(y_pred_prob)\n",
    "    metrics = calculate_evaluation_metrics(y_test, y_pred_prob, 0.95)\n",
    "    print('Score fo/r fold',fold_no,metrics)\n",
    "    fold_no += 1\n",
    "print(calculate(all_y_true, all_y_prob))\n",
    "pd.DataFrame([all_y_true,all_y_prob]).T.to_csv(\"BLACK_cna.csv\")  #H\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基因表达"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv1D, BatchNormalization, Activation, Concatenate, Dropout, Flatten, Dense\n",
    "from tensorflow.keras.initializers import GlorotNormal, Constant\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "def create_gene_model(input_shape):\n",
    "    init = GlorotNormal(seed=1)\n",
    "    bias_init = Constant(value=0.1)\n",
    "    \n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    x = Conv1D(filters=25, kernel_size=2, strides=1, padding=\"same\",\n",
    "               kernel_initializer=init, bias_initializer=bias_init)(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    residual = x\n",
    "\n",
    "    xp1 = Conv1D(filters=10, kernel_size=3, strides=1, padding=\"same\", use_bias=False)(x)\n",
    "    xp1 = BatchNormalization()(xp1)\n",
    "\n",
    "    xp2 = Conv1D(filters=10, kernel_size=3, strides=1, dilation_rate=2, padding=\"same\", use_bias=False)(x)\n",
    "    xp2 = BatchNormalization()(xp2)\n",
    "\n",
    "    x = Concatenate(axis=-1)([xp1, xp2])\n",
    "    x = Dropout(0.5)(x) \n",
    " \n",
    "    # Assuming ExternalAttention is a custom layer that you've defined elsewhere\n",
    "#     x = ExternalAttention(d_model=20)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.25)(x)  \n",
    "\n",
    "    x = x + Conv1D(filters=20, kernel_size=1, strides=1, use_bias=False)(residual)\n",
    "    x = BatchNormalization()(x)\n",
    "   \n",
    "    x = Flatten()(x)\n",
    "\n",
    "    x = Dense(units=300, activation=\"tanh\", kernel_regularizer=l1(0.01))(x)\n",
    "    x = Dropout(0.5)(x)  \n",
    "    outputs = Dense(units=1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    optimizer = Adam(learning_rate=0.0003)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on fold 1...\n",
      "Epoch 1/50\n",
      "7/7 [==============================] - 1s 81ms/step - loss: 255.7282 - accuracy: 0.5882 - val_loss: 238.4705 - val_accuracy: 0.7297\n",
      "Epoch 2/50\n",
      "7/7 [==============================] - 0s 51ms/step - loss: 226.2278 - accuracy: 0.8009 - val_loss: 210.1327 - val_accuracy: 0.7297\n",
      "Epoch 3/50\n",
      "7/7 [==============================] - 0s 51ms/step - loss: 198.5029 - accuracy: 0.8552 - val_loss: 183.4070 - val_accuracy: 0.7568\n",
      "Epoch 4/50\n",
      "7/7 [==============================] - 0s 53ms/step - loss: 172.3825 - accuracy: 0.8914 - val_loss: 158.2881 - val_accuracy: 0.7838\n",
      "Epoch 5/50\n",
      "7/7 [==============================] - 0s 58ms/step - loss: 147.9902 - accuracy: 0.9186 - val_loss: 134.9003 - val_accuracy: 0.7838\n",
      "Epoch 6/50\n",
      "7/7 [==============================] - 0s 54ms/step - loss: 125.3022 - accuracy: 0.9276 - val_loss: 113.3042 - val_accuracy: 0.8108\n",
      "Epoch 7/50\n",
      "7/7 [==============================] - 0s 53ms/step - loss: 104.4676 - accuracy: 0.9412 - val_loss: 93.5795 - val_accuracy: 0.7568\n",
      "Epoch 8/50\n",
      "7/7 [==============================] - 0s 54ms/step - loss: 85.5133 - accuracy: 0.9774 - val_loss: 75.7537 - val_accuracy: 0.7568\n",
      "Epoch 9/50\n",
      "7/7 [==============================] - 0s 57ms/step - loss: 68.4956 - accuracy: 0.9910 - val_loss: 59.8823 - val_accuracy: 0.7297\n",
      "Epoch 10/50\n",
      "7/7 [==============================] - 0s 54ms/step - loss: 53.4526 - accuracy: 0.9864 - val_loss: 45.9803 - val_accuracy: 0.7568\n",
      "Epoch 11/50\n",
      "7/7 [==============================] - 0s 54ms/step - loss: 40.4109 - accuracy: 0.9819 - val_loss: 34.1062 - val_accuracy: 0.7027\n",
      "Epoch 12/50\n",
      "7/7 [==============================] - 0s 56ms/step - loss: 29.3754 - accuracy: 0.9955 - val_loss: 24.2523 - val_accuracy: 0.7568\n",
      "Epoch 13/50\n",
      "7/7 [==============================] - 0s 53ms/step - loss: 20.3661 - accuracy: 0.9955 - val_loss: 16.4224 - val_accuracy: 0.7568\n",
      "Epoch 14/50\n",
      "7/7 [==============================] - 0s 55ms/step - loss: 13.3984 - accuracy: 0.9955 - val_loss: 10.6396 - val_accuracy: 0.7297\n",
      "Epoch 15/50\n",
      "7/7 [==============================] - 0s 52ms/step - loss: 8.5182 - accuracy: 0.9910 - val_loss: 6.9442 - val_accuracy: 0.7568\n",
      "Epoch 16/50\n",
      "7/7 [==============================] - 0s 55ms/step - loss: 5.6461 - accuracy: 0.9955 - val_loss: 4.9641 - val_accuracy: 0.7568\n",
      "Epoch 17/50\n",
      "7/7 [==============================] - 0s 55ms/step - loss: 4.0104 - accuracy: 0.9955 - val_loss: 3.6816 - val_accuracy: 0.7027\n",
      "Epoch 18/50\n",
      "7/7 [==============================] - 0s 50ms/step - loss: 3.0020 - accuracy: 0.9729 - val_loss: 2.9603 - val_accuracy: 0.7297\n",
      "Epoch 19/50\n",
      "7/7 [==============================] - 0s 51ms/step - loss: 2.4827 - accuracy: 0.9729 - val_loss: 2.5715 - val_accuracy: 0.6757\n",
      "Epoch 20/50\n",
      "7/7 [==============================] - 0s 58ms/step - loss: 2.1791 - accuracy: 0.9548 - val_loss: 2.3404 - val_accuracy: 0.7568\n",
      "Epoch 21/50\n",
      "7/7 [==============================] - 0s 62ms/step - loss: 2.0472 - accuracy: 0.9412 - val_loss: 2.2641 - val_accuracy: 0.7838\n",
      "Epoch 22/50\n",
      "7/7 [==============================] - 0s 62ms/step - loss: 1.9458 - accuracy: 0.9638 - val_loss: 2.1662 - val_accuracy: 0.7297\n",
      "Epoch 23/50\n",
      "7/7 [==============================] - 0s 59ms/step - loss: 1.8976 - accuracy: 0.9457 - val_loss: 2.1849 - val_accuracy: 0.7027\n",
      "Epoch 24/50\n",
      "7/7 [==============================] - 0s 61ms/step - loss: 1.9099 - accuracy: 0.9367 - val_loss: 2.1694 - val_accuracy: 0.7568\n",
      "Epoch 25/50\n",
      "7/7 [==============================] - 0s 60ms/step - loss: 1.8493 - accuracy: 0.9638 - val_loss: 2.0973 - val_accuracy: 0.7027\n",
      "Epoch 26/50\n",
      "7/7 [==============================] - 0s 62ms/step - loss: 1.8598 - accuracy: 0.9321 - val_loss: 2.1398 - val_accuracy: 0.7297\n",
      "Epoch 27/50\n",
      "7/7 [==============================] - 0s 59ms/step - loss: 1.8531 - accuracy: 0.9593 - val_loss: 2.1257 - val_accuracy: 0.7297\n",
      "Epoch 28/50\n",
      "7/7 [==============================] - 0s 63ms/step - loss: 1.8310 - accuracy: 0.9457 - val_loss: 2.1100 - val_accuracy: 0.7568\n",
      "Epoch 29/50\n",
      "7/7 [==============================] - 0s 63ms/step - loss: 1.8190 - accuracy: 0.9412 - val_loss: 2.0431 - val_accuracy: 0.7568\n",
      "Epoch 30/50\n",
      "7/7 [==============================] - 0s 59ms/step - loss: 1.8148 - accuracy: 0.9593 - val_loss: 2.0898 - val_accuracy: 0.7297\n",
      "Epoch 31/50\n",
      "7/7 [==============================] - 0s 58ms/step - loss: 1.8129 - accuracy: 0.9457 - val_loss: 2.0730 - val_accuracy: 0.7297\n",
      "Epoch 32/50\n",
      "7/7 [==============================] - 0s 60ms/step - loss: 1.8256 - accuracy: 0.9502 - val_loss: 2.0827 - val_accuracy: 0.7297\n",
      "Epoch 33/50\n",
      "7/7 [==============================] - 0s 59ms/step - loss: 1.7960 - accuracy: 0.9683 - val_loss: 2.0470 - val_accuracy: 0.7568\n",
      "Epoch 34/50\n",
      "7/7 [==============================] - 0s 59ms/step - loss: 1.7499 - accuracy: 0.9548 - val_loss: 1.9927 - val_accuracy: 0.7568\n",
      "Epoch 35/50\n",
      "7/7 [==============================] - 0s 61ms/step - loss: 1.7560 - accuracy: 0.9457 - val_loss: 2.0498 - val_accuracy: 0.7568\n",
      "Epoch 36/50\n",
      "7/7 [==============================] - 0s 56ms/step - loss: 1.7908 - accuracy: 0.9638 - val_loss: 2.0593 - val_accuracy: 0.6757\n",
      "Epoch 37/50\n",
      "7/7 [==============================] - 0s 54ms/step - loss: 1.7758 - accuracy: 0.9186 - val_loss: 2.0482 - val_accuracy: 0.7027\n",
      "Epoch 38/50\n",
      "7/7 [==============================] - 0s 56ms/step - loss: 1.8171 - accuracy: 0.9321 - val_loss: 2.1362 - val_accuracy: 0.7297\n",
      "Epoch 39/50\n",
      "7/7 [==============================] - 0s 57ms/step - loss: 1.8417 - accuracy: 0.9321 - val_loss: 2.1117 - val_accuracy: 0.7838\n",
      "Epoch 40/50\n",
      "7/7 [==============================] - 0s 54ms/step - loss: 1.8099 - accuracy: 0.9683 - val_loss: 2.0348 - val_accuracy: 0.7297\n",
      "Epoch 41/50\n",
      "7/7 [==============================] - 0s 55ms/step - loss: 1.7401 - accuracy: 0.9186 - val_loss: 2.0128 - val_accuracy: 0.7838\n",
      "Epoch 42/50\n",
      "7/7 [==============================] - 0s 62ms/step - loss: 1.7679 - accuracy: 0.9729 - val_loss: 2.0319 - val_accuracy: 0.7297\n",
      "Epoch 43/50\n",
      "7/7 [==============================] - 0s 60ms/step - loss: 1.7199 - accuracy: 0.9457 - val_loss: 2.0024 - val_accuracy: 0.7027\n",
      "Epoch 44/50\n",
      "7/7 [==============================] - 0s 62ms/step - loss: 1.7161 - accuracy: 0.9412 - val_loss: 1.9516 - val_accuracy: 0.7297\n",
      "Epoch 45/50\n",
      "7/7 [==============================] - 0s 65ms/step - loss: 1.7012 - accuracy: 0.9321 - val_loss: 1.9932 - val_accuracy: 0.7297\n",
      "Epoch 46/50\n",
      "7/7 [==============================] - 0s 52ms/step - loss: 1.7358 - accuracy: 0.9593 - val_loss: 1.9875 - val_accuracy: 0.7568\n",
      "Epoch 47/50\n",
      "7/7 [==============================] - 0s 52ms/step - loss: 1.7298 - accuracy: 0.9638 - val_loss: 2.0092 - val_accuracy: 0.7568\n",
      "Epoch 48/50\n",
      "7/7 [==============================] - 0s 58ms/step - loss: 1.7145 - accuracy: 0.9548 - val_loss: 1.9874 - val_accuracy: 0.7297\n",
      "Epoch 49/50\n",
      "7/7 [==============================] - 0s 52ms/step - loss: 1.7128 - accuracy: 0.9683 - val_loss: 2.0333 - val_accuracy: 0.7027\n",
      "Epoch 50/50\n",
      "7/7 [==============================] - 0s 56ms/step - loss: 1.7034 - accuracy: 0.9548 - val_loss: 1.9566 - val_accuracy: 0.7838\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000021F13F16048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Score fo/r fold 1 (0.7567567567567568, 0.6785714285714286, 0.0, 0.0, 0.0, 1.6821771)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "E:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\_classification.py:873: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on fold 2...\n",
      "Epoch 1/50\n",
      "8/8 [==============================] - 1s 76ms/step - loss: 255.3427 - accuracy: 0.6089 - val_loss: 234.4077 - val_accuracy: 0.8056\n",
      "Epoch 2/50\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 223.3267 - accuracy: 0.7689 - val_loss: 205.3647 - val_accuracy: 0.7778\n",
      "Epoch 3/50\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 194.5492 - accuracy: 0.8711 - val_loss: 177.4486 - val_accuracy: 0.7500\n",
      "Epoch 4/50\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 167.1362 - accuracy: 0.9156 - val_loss: 151.1384 - val_accuracy: 0.7222\n",
      "Epoch 5/50\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 141.5153 - accuracy: 0.9067 - val_loss: 126.7818 - val_accuracy: 0.7222\n",
      "Epoch 6/50\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 117.8704 - accuracy: 0.9467 - val_loss: 104.4952 - val_accuracy: 0.7778\n",
      "Epoch 7/50\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 96.4971 - accuracy: 0.9378 - val_loss: 84.7131 - val_accuracy: 0.7222\n",
      "Epoch 8/50\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 77.9029 - accuracy: 0.9600 - val_loss: 67.9133 - val_accuracy: 0.6667\n",
      "Epoch 9/50\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 61.8032 - accuracy: 0.9467 - val_loss: 53.0962 - val_accuracy: 0.6111\n",
      "Epoch 10/50\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 47.6396 - accuracy: 0.9644 - val_loss: 40.4062 - val_accuracy: 0.6389\n",
      "Epoch 11/50\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 36.2481 - accuracy: 0.9644 - val_loss: 30.6894 - val_accuracy: 0.6944\n",
      "Epoch 12/50\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 27.1691 - accuracy: 0.9733 - val_loss: 22.7594 - val_accuracy: 0.7500\n",
      "Epoch 13/50\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 20.0326 - accuracy: 0.9733 - val_loss: 17.0021 - val_accuracy: 0.6667\n",
      "Epoch 14/50\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 15.0213 - accuracy: 0.9689 - val_loss: 13.1091 - val_accuracy: 0.6667\n",
      "Epoch 15/50\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 11.5194 - accuracy: 0.9822 - val_loss: 10.2857 - val_accuracy: 0.6667\n",
      "Epoch 16/50\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 9.2675 - accuracy: 0.9689 - val_loss: 8.5427 - val_accuracy: 0.6667\n",
      "Epoch 17/50\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 7.5135 - accuracy: 0.9822 - val_loss: 7.1077 - val_accuracy: 0.6944\n",
      "Epoch 18/50\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 6.5612 - accuracy: 0.9689 - val_loss: 6.4238 - val_accuracy: 0.5833\n",
      "Epoch 19/50\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 5.6674 - accuracy: 0.9733 - val_loss: 5.5334 - val_accuracy: 0.6389\n",
      "Epoch 20/50\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 5.1188 - accuracy: 0.9600 - val_loss: 5.2374 - val_accuracy: 0.5833\n",
      "Epoch 21/50\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 4.8496 - accuracy: 0.9156 - val_loss: 4.9323 - val_accuracy: 0.6389\n",
      "Epoch 22/50\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 4.5108 - accuracy: 0.9689 - val_loss: 4.6698 - val_accuracy: 0.6667\n",
      "Epoch 23/50\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 4.3920 - accuracy: 0.8800 - val_loss: 4.5323 - val_accuracy: 0.7500\n",
      "Epoch 24/50\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 4.3456 - accuracy: 0.9733 - val_loss: 4.4400 - val_accuracy: 0.6667\n",
      "Epoch 25/50\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 3.8790 - accuracy: 0.9511 - val_loss: 3.9313 - val_accuracy: 0.6389\n",
      "Epoch 26/50\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 3.7455 - accuracy: 0.9156 - val_loss: 4.0401 - val_accuracy: 0.7500\n",
      "Epoch 27/50\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 4.0500 - accuracy: 0.9244 - val_loss: 4.4628 - val_accuracy: 0.5000\n",
      "Epoch 28/50\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 3.9020 - accuracy: 0.9200 - val_loss: 3.9883 - val_accuracy: 0.5556\n",
      "Epoch 29/50\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 3.6345 - accuracy: 0.9289 - val_loss: 3.6328 - val_accuracy: 0.5833\n",
      "Epoch 30/50\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 2.9725 - accuracy: 0.9689 - val_loss: 3.1742 - val_accuracy: 0.5278\n",
      "Epoch 31/50\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 3.0938 - accuracy: 0.8756 - val_loss: 3.4179 - val_accuracy: 0.5833\n",
      "Epoch 32/50\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 3.0853 - accuracy: 0.9200 - val_loss: 3.1506 - val_accuracy: 0.6389\n",
      "Epoch 33/50\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 2.6769 - accuracy: 0.9467 - val_loss: 2.9035 - val_accuracy: 0.6667\n",
      "Epoch 34/50\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 2.9534 - accuracy: 0.8978 - val_loss: 3.3034 - val_accuracy: 0.5833\n",
      "Epoch 35/50\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 3.0037 - accuracy: 0.9244 - val_loss: 3.1562 - val_accuracy: 0.6944\n",
      "Epoch 36/50\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 3.0542 - accuracy: 0.8978 - val_loss: 3.2650 - val_accuracy: 0.5556\n",
      "Epoch 37/50\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 2.6912 - accuracy: 0.9200 - val_loss: 2.6980 - val_accuracy: 0.6944\n",
      "Epoch 38/50\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 2.3124 - accuracy: 0.9467 - val_loss: 2.5045 - val_accuracy: 0.6944\n",
      "Epoch 39/50\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 2.3007 - accuracy: 0.9200 - val_loss: 2.5790 - val_accuracy: 0.6944\n",
      "Epoch 40/50\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 2.3329 - accuracy: 0.9378 - val_loss: 2.5349 - val_accuracy: 0.7222\n",
      "Epoch 41/50\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 2.3998 - accuracy: 0.9200 - val_loss: 2.7739 - val_accuracy: 0.6944\n",
      "Epoch 42/50\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 2.7786 - accuracy: 0.9244 - val_loss: 3.0394 - val_accuracy: 0.6667\n",
      "Epoch 43/50\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 2.5240 - accuracy: 0.9022 - val_loss: 2.6097 - val_accuracy: 0.6667\n",
      "Epoch 44/50\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 2.3346 - accuracy: 0.9422 - val_loss: 2.6573 - val_accuracy: 0.7778\n",
      "Epoch 45/50\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 3.1933 - accuracy: 0.8711 - val_loss: 3.6318 - val_accuracy: 0.7500\n",
      "Epoch 46/50\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 3.2955 - accuracy: 0.9156 - val_loss: 3.1554 - val_accuracy: 0.8056\n",
      "Epoch 47/50\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 2.7158 - accuracy: 0.9511 - val_loss: 2.7076 - val_accuracy: 0.7500\n",
      "Epoch 48/50\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 2.3887 - accuracy: 0.9600 - val_loss: 2.7717 - val_accuracy: 0.6944\n",
      "Epoch 49/50\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 2.8300 - accuracy: 0.8578 - val_loss: 3.6386 - val_accuracy: 0.4167\n",
      "Epoch 50/50\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 3.7982 - accuracy: 0.8000 - val_loss: 4.2845 - val_accuracy: 0.6389\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000021F27AA5BF8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Score fo/r fold 2 (0.75, 0.4464285714285714, 0.0, 0.0, -0.09035079029052513, 0.80907774)\n",
      "Training on fold 3...\n",
      "Epoch 1/50\n",
      "8/8 [==============================] - 1s 73ms/step - loss: 255.5258 - accuracy: 0.5242 - val_loss: 234.6424 - val_accuracy: 0.7778\n",
      "Epoch 2/50\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 222.7729 - accuracy: 0.7797 - val_loss: 203.8591 - val_accuracy: 0.7778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 192.5876 - accuracy: 0.8767 - val_loss: 175.0455 - val_accuracy: 0.7778\n",
      "Epoch 4/50\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 164.5420 - accuracy: 0.8899 - val_loss: 148.3938 - val_accuracy: 0.8611\n",
      "Epoch 5/50\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 138.6503 - accuracy: 0.9251 - val_loss: 123.9042 - val_accuracy: 0.8333\n",
      "Epoch 6/50\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 114.8604 - accuracy: 0.9339 - val_loss: 101.4435 - val_accuracy: 0.7500\n",
      "Epoch 7/50\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 93.1825 - accuracy: 0.9736 - val_loss: 81.2159 - val_accuracy: 0.7222\n",
      "Epoch 8/50\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 73.8211 - accuracy: 0.9736 - val_loss: 63.3355 - val_accuracy: 0.6944\n",
      "Epoch 9/50\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 56.8817 - accuracy: 0.9604 - val_loss: 47.9215 - val_accuracy: 0.6111\n",
      "Epoch 10/50\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 42.3498 - accuracy: 0.9868 - val_loss: 34.9029 - val_accuracy: 0.5556\n",
      "Epoch 11/50\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 30.2487 - accuracy: 0.9780 - val_loss: 24.3716 - val_accuracy: 0.5833\n",
      "Epoch 12/50\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 20.7112 - accuracy: 0.9912 - val_loss: 16.3944 - val_accuracy: 0.6111\n",
      "Epoch 13/50\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 13.6528 - accuracy: 0.9912 - val_loss: 10.9069 - val_accuracy: 0.5833\n",
      "Epoch 14/50\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 9.0884 - accuracy: 0.9912 - val_loss: 7.6556 - val_accuracy: 0.5833\n",
      "Epoch 15/50\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 6.4530 - accuracy: 0.9780 - val_loss: 5.6672 - val_accuracy: 0.5556\n",
      "Epoch 16/50\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 4.8083 - accuracy: 0.9868 - val_loss: 4.4663 - val_accuracy: 0.5000\n",
      "Epoch 17/50\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 3.7375 - accuracy: 0.9868 - val_loss: 3.6424 - val_accuracy: 0.6111\n",
      "Epoch 18/50\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 3.1828 - accuracy: 0.9559 - val_loss: 3.3249 - val_accuracy: 0.6111\n",
      "Epoch 19/50\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 3.1895 - accuracy: 0.9295 - val_loss: 3.4805 - val_accuracy: 0.4722\n",
      "Epoch 20/50\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 2.9460 - accuracy: 0.9031 - val_loss: 3.0437 - val_accuracy: 0.5278\n",
      "Epoch 21/50\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 2.7670 - accuracy: 0.9295 - val_loss: 3.0536 - val_accuracy: 0.5278\n",
      "Epoch 22/50\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 2.7747 - accuracy: 0.9339 - val_loss: 2.9866 - val_accuracy: 0.5833\n",
      "Epoch 23/50\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 2.6561 - accuracy: 0.9339 - val_loss: 2.8273 - val_accuracy: 0.6389\n",
      "Epoch 24/50\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 2.4824 - accuracy: 0.9251 - val_loss: 2.6703 - val_accuracy: 0.6667\n",
      "Epoch 25/50\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 2.4951 - accuracy: 0.9163 - val_loss: 2.7655 - val_accuracy: 0.6667\n",
      "Epoch 26/50\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 2.5940 - accuracy: 0.9471 - val_loss: 2.8492 - val_accuracy: 0.5278\n",
      "Epoch 27/50\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 2.4876 - accuracy: 0.9471 - val_loss: 2.6795 - val_accuracy: 0.5556\n",
      "Epoch 28/50\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 2.2097 - accuracy: 0.9427 - val_loss: 2.4335 - val_accuracy: 0.5278\n",
      "Epoch 29/50\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 2.1258 - accuracy: 0.9207 - val_loss: 2.4317 - val_accuracy: 0.5556\n",
      "Epoch 30/50\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 2.2278 - accuracy: 0.9075 - val_loss: 2.5466 - val_accuracy: 0.6389\n",
      "Epoch 31/50\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 2.4369 - accuracy: 0.8899 - val_loss: 2.7441 - val_accuracy: 0.6944\n",
      "Epoch 32/50\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 2.6010 - accuracy: 0.9427 - val_loss: 2.7883 - val_accuracy: 0.6667\n",
      "Epoch 33/50\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 2.4336 - accuracy: 0.9515 - val_loss: 2.6499 - val_accuracy: 0.5833\n",
      "Epoch 34/50\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 2.3960 - accuracy: 0.9119 - val_loss: 2.6736 - val_accuracy: 0.6111\n",
      "Epoch 35/50\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 2.4631 - accuracy: 0.9427 - val_loss: 2.7310 - val_accuracy: 0.5833\n",
      "Epoch 36/50\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 2.4295 - accuracy: 0.9604 - val_loss: 2.7191 - val_accuracy: 0.6667\n",
      "Epoch 37/50\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 2.7080 - accuracy: 0.9251 - val_loss: 3.1966 - val_accuracy: 0.6944\n",
      "Epoch 38/50\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 3.4016 - accuracy: 0.9163 - val_loss: 3.9428 - val_accuracy: 0.5556\n",
      "Epoch 39/50\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 3.4396 - accuracy: 0.9251 - val_loss: 3.4557 - val_accuracy: 0.5000\n",
      "Epoch 40/50\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 2.8189 - accuracy: 0.9339 - val_loss: 2.9172 - val_accuracy: 0.5000\n",
      "Epoch 41/50\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 2.5430 - accuracy: 0.9427 - val_loss: 2.7019 - val_accuracy: 0.6667\n",
      "Epoch 42/50\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 2.2645 - accuracy: 0.9427 - val_loss: 2.4498 - val_accuracy: 0.7500\n",
      "Epoch 43/50\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 2.3029 - accuracy: 0.9427 - val_loss: 2.6576 - val_accuracy: 0.6944\n",
      "Epoch 44/50\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 2.4670 - accuracy: 0.9515 - val_loss: 2.6862 - val_accuracy: 0.5556\n",
      "Epoch 45/50\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 2.2168 - accuracy: 0.9383 - val_loss: 2.3971 - val_accuracy: 0.7500\n",
      "Epoch 46/50\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 2.3442 - accuracy: 0.9383 - val_loss: 2.6487 - val_accuracy: 0.6944\n",
      "Epoch 47/50\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 2.2739 - accuracy: 0.9604 - val_loss: 2.5399 - val_accuracy: 0.5556\n",
      "Epoch 48/50\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 2.3921 - accuracy: 0.9075 - val_loss: 2.6591 - val_accuracy: 0.7778\n",
      "Epoch 49/50\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 2.3763 - accuracy: 0.9604 - val_loss: 2.5235 - val_accuracy: 0.7778\n",
      "Epoch 50/50\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 2.4083 - accuracy: 0.9119 - val_loss: 2.5751 - val_accuracy: 0.6944\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000021F08212840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Score fo/r fold 3 (0.8055555555555556, 0.7767857142857142, 0.6666666666666666, 0.25, 0.3223291856101521, 0.67516226)\n",
      "Training on fold 4...\n",
      "Epoch 1/50\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 255.5978 - accuracy: 0.5837 - val_loss: 238.2260 - val_accuracy: 0.7778\n",
      "Epoch 2/50\n",
      "7/7 [==============================] - 0s 51ms/step - loss: 226.1719 - accuracy: 0.7330 - val_loss: 210.0525 - val_accuracy: 0.7778\n",
      "Epoch 3/50\n",
      "7/7 [==============================] - 0s 69ms/step - loss: 198.5877 - accuracy: 0.8688 - val_loss: 183.4687 - val_accuracy: 0.8056\n",
      "Epoch 4/50\n",
      "7/7 [==============================] - 0s 67ms/step - loss: 172.5509 - accuracy: 0.9140 - val_loss: 158.4552 - val_accuracy: 0.8611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50\n",
      "7/7 [==============================] - 0s 64ms/step - loss: 148.1448 - accuracy: 0.9412 - val_loss: 135.0425 - val_accuracy: 0.8333\n",
      "Epoch 6/50\n",
      "7/7 [==============================] - 0s 60ms/step - loss: 125.4909 - accuracy: 0.9321 - val_loss: 113.4665 - val_accuracy: 0.8056\n",
      "Epoch 7/50\n",
      "7/7 [==============================] - 1s 81ms/step - loss: 104.6521 - accuracy: 0.9412 - val_loss: 93.7618 - val_accuracy: 0.8056\n",
      "Epoch 8/50\n",
      "7/7 [==============================] - 0s 58ms/step - loss: 85.7126 - accuracy: 0.9593 - val_loss: 75.9611 - val_accuracy: 0.8056\n",
      "Epoch 9/50\n",
      "7/7 [==============================] - 0s 63ms/step - loss: 68.7226 - accuracy: 0.9548 - val_loss: 60.1322 - val_accuracy: 0.7778\n",
      "Epoch 10/50\n",
      "7/7 [==============================] - 0s 56ms/step - loss: 53.6846 - accuracy: 0.9729 - val_loss: 46.2448 - val_accuracy: 0.7778\n",
      "Epoch 11/50\n",
      "7/7 [==============================] - 0s 64ms/step - loss: 40.6261 - accuracy: 0.9819 - val_loss: 34.3672 - val_accuracy: 0.7222\n",
      "Epoch 12/50\n",
      "7/7 [==============================] - 0s 62ms/step - loss: 29.5561 - accuracy: 1.0000 - val_loss: 24.4701 - val_accuracy: 0.7500\n",
      "Epoch 13/50\n",
      "7/7 [==============================] - 0s 59ms/step - loss: 20.5544 - accuracy: 0.9864 - val_loss: 16.6518 - val_accuracy: 0.6667\n",
      "Epoch 14/50\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 13.5566 - accuracy: 1.0000 - val_loss: 10.8337 - val_accuracy: 0.6944\n",
      "Epoch 15/50\n",
      "7/7 [==============================] - 0s 53ms/step - loss: 8.6340 - accuracy: 0.9955 - val_loss: 7.0882 - val_accuracy: 0.6667\n",
      "Epoch 16/50\n",
      "7/7 [==============================] - 0s 46ms/step - loss: 5.7029 - accuracy: 0.9955 - val_loss: 5.0131 - val_accuracy: 0.7500\n",
      "Epoch 17/50\n",
      "7/7 [==============================] - 0s 46ms/step - loss: 3.9937 - accuracy: 0.9955 - val_loss: 3.6773 - val_accuracy: 0.7222\n",
      "Epoch 18/50\n",
      "7/7 [==============================] - 0s 63ms/step - loss: 2.9730 - accuracy: 0.9910 - val_loss: 2.9800 - val_accuracy: 0.6944\n",
      "Epoch 19/50\n",
      "7/7 [==============================] - 0s 51ms/step - loss: 2.4412 - accuracy: 0.9638 - val_loss: 2.5713 - val_accuracy: 0.6944\n",
      "Epoch 20/50\n",
      "7/7 [==============================] - 0s 48ms/step - loss: 2.1897 - accuracy: 0.9638 - val_loss: 2.3910 - val_accuracy: 0.6667\n",
      "Epoch 21/50\n",
      "7/7 [==============================] - 0s 47ms/step - loss: 2.0559 - accuracy: 0.9593 - val_loss: 2.2967 - val_accuracy: 0.6389\n",
      "Epoch 22/50\n",
      "7/7 [==============================] - 0s 45ms/step - loss: 1.9780 - accuracy: 0.9548 - val_loss: 2.2683 - val_accuracy: 0.6667\n",
      "Epoch 23/50\n",
      "7/7 [==============================] - 0s 61ms/step - loss: 1.9542 - accuracy: 0.9321 - val_loss: 2.3020 - val_accuracy: 0.5556\n",
      "Epoch 24/50\n",
      "7/7 [==============================] - 0s 53ms/step - loss: 1.9655 - accuracy: 0.9457 - val_loss: 2.2346 - val_accuracy: 0.7222\n",
      "Epoch 25/50\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 1.9232 - accuracy: 0.9502 - val_loss: 2.2525 - val_accuracy: 0.5278\n",
      "Epoch 26/50\n",
      "7/7 [==============================] - 0s 55ms/step - loss: 1.9334 - accuracy: 0.9276 - val_loss: 2.2340 - val_accuracy: 0.6389\n",
      "Epoch 27/50\n",
      "7/7 [==============================] - 0s 50ms/step - loss: 1.8942 - accuracy: 0.9412 - val_loss: 2.2256 - val_accuracy: 0.5833\n",
      "Epoch 28/50\n",
      "7/7 [==============================] - 0s 48ms/step - loss: 1.8634 - accuracy: 0.9548 - val_loss: 2.1635 - val_accuracy: 0.6944\n",
      "Epoch 29/50\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 1.8474 - accuracy: 0.9457 - val_loss: 2.1850 - val_accuracy: 0.7778\n",
      "Epoch 30/50\n",
      "7/7 [==============================] - 0s 53ms/step - loss: 1.8620 - accuracy: 0.9502 - val_loss: 2.1854 - val_accuracy: 0.6111\n",
      "Epoch 31/50\n",
      "7/7 [==============================] - 0s 47ms/step - loss: 1.8857 - accuracy: 0.9367 - val_loss: 2.2393 - val_accuracy: 0.6389\n",
      "Epoch 32/50\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 1.8703 - accuracy: 0.9502 - val_loss: 2.1630 - val_accuracy: 0.7778\n",
      "Epoch 33/50\n",
      "7/7 [==============================] - 0s 47ms/step - loss: 1.7878 - accuracy: 0.9774 - val_loss: 2.1200 - val_accuracy: 0.6389\n",
      "Epoch 34/50\n",
      "7/7 [==============================] - 0s 56ms/step - loss: 1.7761 - accuracy: 0.9548 - val_loss: 2.1023 - val_accuracy: 0.7222\n",
      "Epoch 35/50\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 1.8176 - accuracy: 0.9457 - val_loss: 2.2043 - val_accuracy: 0.6389\n",
      "Epoch 36/50\n",
      "7/7 [==============================] - 0s 54ms/step - loss: 1.8251 - accuracy: 0.9457 - val_loss: 2.0792 - val_accuracy: 0.8056\n",
      "Epoch 37/50\n",
      "7/7 [==============================] - 0s 57ms/step - loss: 1.7979 - accuracy: 0.9548 - val_loss: 2.0801 - val_accuracy: 0.6944\n",
      "Epoch 38/50\n",
      "7/7 [==============================] - 0s 48ms/step - loss: 1.7800 - accuracy: 0.9638 - val_loss: 2.0926 - val_accuracy: 0.7778\n",
      "Epoch 39/50\n",
      "7/7 [==============================] - 0s 46ms/step - loss: 1.7991 - accuracy: 0.9502 - val_loss: 2.1787 - val_accuracy: 0.6667\n",
      "Epoch 40/50\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 1.7815 - accuracy: 0.9548 - val_loss: 2.0299 - val_accuracy: 0.7778\n",
      "Epoch 41/50\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 1.7766 - accuracy: 0.9593 - val_loss: 2.1306 - val_accuracy: 0.7500\n",
      "Epoch 42/50\n",
      "7/7 [==============================] - 0s 53ms/step - loss: 1.7730 - accuracy: 0.9548 - val_loss: 2.1013 - val_accuracy: 0.7778\n",
      "Epoch 43/50\n",
      "7/7 [==============================] - 0s 54ms/step - loss: 1.7636 - accuracy: 0.9548 - val_loss: 2.1492 - val_accuracy: 0.6944\n",
      "Epoch 44/50\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 1.7717 - accuracy: 0.9457 - val_loss: 2.0857 - val_accuracy: 0.7222\n",
      "Epoch 45/50\n",
      "7/7 [==============================] - 0s 63ms/step - loss: 1.7448 - accuracy: 0.9367 - val_loss: 2.0336 - val_accuracy: 0.7500\n",
      "Epoch 46/50\n",
      "7/7 [==============================] - 0s 52ms/step - loss: 1.7671 - accuracy: 0.9412 - val_loss: 2.1827 - val_accuracy: 0.6944\n",
      "Epoch 47/50\n",
      "7/7 [==============================] - 0s 54ms/step - loss: 1.7939 - accuracy: 0.9729 - val_loss: 2.0494 - val_accuracy: 0.7778\n",
      "Epoch 48/50\n",
      "7/7 [==============================] - 0s 53ms/step - loss: 1.7459 - accuracy: 0.9502 - val_loss: 2.0105 - val_accuracy: 0.7778\n",
      "Epoch 49/50\n",
      "7/7 [==============================] - 0s 52ms/step - loss: 1.7300 - accuracy: 0.9638 - val_loss: 2.0864 - val_accuracy: 0.7222\n",
      "Epoch 50/50\n",
      "7/7 [==============================] - 0s 61ms/step - loss: 1.6986 - accuracy: 0.9774 - val_loss: 1.9552 - val_accuracy: 0.7778\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000021F6DEF1EA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Score fo/r fold 4 (0.8055555555555556, 0.7321428571428571, 0.6666666666666666, 0.25, 0.3223291856101521, 0.66809213)\n",
      "Training on fold 5...\n",
      "Epoch 1/50\n",
      "8/8 [==============================] - 1s 78ms/step - loss: 255.5899 - accuracy: 0.5422 - val_loss: 234.7438 - val_accuracy: 0.7500\n",
      "Epoch 2/50\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 223.5133 - accuracy: 0.7556 - val_loss: 205.2237 - val_accuracy: 0.7500\n",
      "Epoch 3/50\n",
      "8/8 [==============================] - 1s 66ms/step - loss: 194.2250 - accuracy: 0.8622 - val_loss: 177.1331 - val_accuracy: 0.7222\n",
      "Epoch 4/50\n",
      "8/8 [==============================] - 1s 72ms/step - loss: 167.2403 - accuracy: 0.9022 - val_loss: 151.9040 - val_accuracy: 0.7778\n",
      "Epoch 5/50\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 142.4973 - accuracy: 0.9289 - val_loss: 128.1515 - val_accuracy: 0.7778\n",
      "Epoch 6/50\n",
      "8/8 [==============================] - 1s 82ms/step - loss: 119.4052 - accuracy: 0.9378 - val_loss: 106.3124 - val_accuracy: 0.7222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 98.2944 - accuracy: 0.9689 - val_loss: 86.7614 - val_accuracy: 0.6667\n",
      "Epoch 8/50\n",
      "8/8 [==============================] - 0s 55ms/step - loss: 80.2183 - accuracy: 0.9156 - val_loss: 70.6545 - val_accuracy: 0.5833\n",
      "Epoch 9/50\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 64.4743 - accuracy: 0.9644 - val_loss: 55.8705 - val_accuracy: 0.6389\n",
      "Epoch 10/50\n",
      "8/8 [==============================] - 0s 54ms/step - loss: 50.4209 - accuracy: 0.9511 - val_loss: 43.1434 - val_accuracy: 0.6111\n",
      "Epoch 11/50\n",
      "8/8 [==============================] - 1s 66ms/step - loss: 38.4768 - accuracy: 0.9778 - val_loss: 32.5976 - val_accuracy: 0.6389\n",
      "Epoch 12/50\n",
      "8/8 [==============================] - 1s 71ms/step - loss: 28.8378 - accuracy: 0.9822 - val_loss: 24.3728 - val_accuracy: 0.6111\n",
      "Epoch 13/50\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 21.4013 - accuracy: 0.9867 - val_loss: 18.2769 - val_accuracy: 0.6111\n",
      "Epoch 14/50\n",
      "8/8 [==============================] - 1s 70ms/step - loss: 16.1361 - accuracy: 0.9822 - val_loss: 14.2011 - val_accuracy: 0.6389\n",
      "Epoch 15/50\n",
      "8/8 [==============================] - 1s 67ms/step - loss: 12.5254 - accuracy: 0.9956 - val_loss: 11.1753 - val_accuracy: 0.6389\n",
      "Epoch 16/50\n",
      "8/8 [==============================] - 1s 66ms/step - loss: 9.8884 - accuracy: 0.9822 - val_loss: 9.0472 - val_accuracy: 0.6111\n",
      "Epoch 17/50\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 8.0049 - accuracy: 0.9822 - val_loss: 7.5251 - val_accuracy: 0.6667\n",
      "Epoch 18/50\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 6.8298 - accuracy: 0.9689 - val_loss: 6.6283 - val_accuracy: 0.6389\n",
      "Epoch 19/50\n",
      "8/8 [==============================] - 1s 63ms/step - loss: 6.0216 - accuracy: 0.9644 - val_loss: 5.8786 - val_accuracy: 0.5278\n",
      "Epoch 20/50\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 5.1150 - accuracy: 0.9733 - val_loss: 5.3062 - val_accuracy: 0.5278\n",
      "Epoch 21/50\n",
      "8/8 [==============================] - 1s 67ms/step - loss: 5.3518 - accuracy: 0.9156 - val_loss: 5.6966 - val_accuracy: 0.6389\n",
      "Epoch 22/50\n",
      "8/8 [==============================] - 1s 67ms/step - loss: 5.3049 - accuracy: 0.9289 - val_loss: 5.2003 - val_accuracy: 0.6667\n",
      "Epoch 23/50\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 4.5543 - accuracy: 0.9644 - val_loss: 4.3480 - val_accuracy: 0.6389\n",
      "Epoch 24/50\n",
      "8/8 [==============================] - 0s 54ms/step - loss: 3.7404 - accuracy: 0.9733 - val_loss: 4.0508 - val_accuracy: 0.5556\n",
      "Epoch 25/50\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 4.2059 - accuracy: 0.9022 - val_loss: 4.7077 - val_accuracy: 0.5833\n",
      "Epoch 26/50\n",
      "8/8 [==============================] - 1s 71ms/step - loss: 4.4298 - accuracy: 0.8844 - val_loss: 4.5414 - val_accuracy: 0.6667\n",
      "Epoch 27/50\n",
      "8/8 [==============================] - 1s 63ms/step - loss: 3.8570 - accuracy: 0.9467 - val_loss: 3.9733 - val_accuracy: 0.4722\n",
      "Epoch 28/50\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 3.6842 - accuracy: 0.9067 - val_loss: 3.8283 - val_accuracy: 0.6111\n",
      "Epoch 29/50\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 3.1941 - accuracy: 0.9511 - val_loss: 3.2670 - val_accuracy: 0.7222\n",
      "Epoch 30/50\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 3.1593 - accuracy: 0.9289 - val_loss: 3.4780 - val_accuracy: 0.6389\n",
      "Epoch 31/50\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 3.4534 - accuracy: 0.8756 - val_loss: 3.7240 - val_accuracy: 0.5278\n",
      "Epoch 32/50\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 3.0967 - accuracy: 0.9200 - val_loss: 3.1182 - val_accuracy: 0.7778\n",
      "Epoch 33/50\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 2.9977 - accuracy: 0.9200 - val_loss: 3.1677 - val_accuracy: 0.6111\n",
      "Epoch 34/50\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 2.7436 - accuracy: 0.9244 - val_loss: 3.0526 - val_accuracy: 0.8056\n",
      "Epoch 35/50\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 3.6173 - accuracy: 0.8978 - val_loss: 4.1717 - val_accuracy: 0.6111\n",
      "Epoch 36/50\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 3.6582 - accuracy: 0.9156 - val_loss: 3.5701 - val_accuracy: 0.5556\n",
      "Epoch 37/50\n",
      "8/8 [==============================] - 0s 55ms/step - loss: 2.8988 - accuracy: 0.9644 - val_loss: 3.0292 - val_accuracy: 0.6111\n",
      "Epoch 38/50\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 2.9952 - accuracy: 0.8800 - val_loss: 3.2463 - val_accuracy: 0.6389\n",
      "Epoch 39/50\n",
      "8/8 [==============================] - 1s 67ms/step - loss: 2.7602 - accuracy: 0.9556 - val_loss: 2.7548 - val_accuracy: 0.7500\n",
      "Epoch 40/50\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 2.3667 - accuracy: 0.9733 - val_loss: 2.6050 - val_accuracy: 0.6944\n",
      "Epoch 41/50\n",
      "8/8 [==============================] - 1s 64ms/step - loss: 2.5478 - accuracy: 0.9156 - val_loss: 3.1424 - val_accuracy: 0.6944\n",
      "Epoch 42/50\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 3.5096 - accuracy: 0.8622 - val_loss: 4.4786 - val_accuracy: 0.5000\n",
      "Epoch 43/50\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 5.1563 - accuracy: 0.8356 - val_loss: 6.2601 - val_accuracy: 0.6389\n",
      "Epoch 44/50\n",
      "8/8 [==============================] - 1s 64ms/step - loss: 5.7974 - accuracy: 0.9244 - val_loss: 6.0267 - val_accuracy: 0.6111\n",
      "Epoch 45/50\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 5.4473 - accuracy: 0.9422 - val_loss: 5.5685 - val_accuracy: 0.6111\n",
      "Epoch 46/50\n",
      "8/8 [==============================] - 1s 72ms/step - loss: 4.8940 - accuracy: 0.9422 - val_loss: 5.0597 - val_accuracy: 0.6111\n",
      "Epoch 47/50\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 4.5113 - accuracy: 0.9289 - val_loss: 4.8278 - val_accuracy: 0.5833\n",
      "Epoch 48/50\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 4.1320 - accuracy: 0.9511 - val_loss: 4.3195 - val_accuracy: 0.6111\n",
      "Epoch 49/50\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 3.6032 - accuracy: 0.9422 - val_loss: 3.6570 - val_accuracy: 0.6667\n",
      "Epoch 50/50\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 3.1315 - accuracy: 0.9644 - val_loss: 3.2492 - val_accuracy: 0.6389\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000021F083EBE18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Score fo/r fold 5 (0.7222222222222222, 0.654320987654321, 0.0, 0.0, -0.09759000729485333, 0.861332)\n",
      "(0.7679558011049724, 0.6421719767043508, 0.5, 0.16666666666666666, 0.18378729065035482, 0.66809213)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "fold_no = 1\n",
    "scores = []\n",
    "all_y_true = []\n",
    "all_y_prob = []\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "skf = StratifiedKFold(n_splits=5,shuffle=True, random_state=42)\n",
    "all_fold_histories = []\n",
    "for i in range(5):  \n",
    "    with open(f'BLACK_train_index_fold{i}.pkl', 'rb') as f:\n",
    "        train_index = pickle.load(f)\n",
    "    with open(f'BLACK_test_index_fold{i}.pkl', 'rb') as f:\n",
    "        test_index = pickle.load(f)\n",
    "    \n",
    "    X_train, X_test = TCGA_gene[train_index], TCGA_gene[test_index]\n",
    "    y_train, y_test = TCGA_label[train_index], TCGA_label[test_index]\n",
    "    \n",
    "    smote_enn = ADASYN(random_state=42)\n",
    "\n",
    "    X_train, y_train = smote_enn.fit_resample(X_train, y_train)\n",
    "    \n",
    "    model = create_gene_model(input_shape = (280, 1))\n",
    "    print(f'Training on fold {fold_no}...')\n",
    "    history = model.fit(X_train, y_train,validation_data=(X_test,y_test), epochs=50, batch_size=32, verbose=1)\n",
    "    \n",
    "    # 评估模型\n",
    "    all_fold_histories.append(history.history)\n",
    "    scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "    y_pred_prob = model.predict(X_test).reshape(-1)\n",
    "    all_y_true.extend(y_test)\n",
    "    all_y_prob.extend(y_pred_prob)\n",
    "    metrics = calculate_evaluation_metrics(y_test, y_pred_prob, 0.95)\n",
    "    print('Score fo/r fold',fold_no,metrics)\n",
    "    fold_no += 1\n",
    "print(calculate_evaluation_metrics(all_y_true, all_y_prob, 0.95))\n",
    "pd.DataFrame([all_y_true,all_y_prob]).T.to_csv(\"BLACK_gene.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:lm] *",
   "language": "python",
   "name": "conda-env-lm-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
